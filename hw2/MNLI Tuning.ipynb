{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import basic_conf as conf\n",
    "from libs import ModelManager as mm\n",
    "from config.constants import HyperParamKey, LoadingKey\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:12:31] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-28 14:12:31] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "NLICNN\n",
      "========================\n",
      "[2018-10-28 14:12:31] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "MNLI\n",
      "========================\n",
      "[2018-10-28 14:12:31] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7f04a1d92ae8>\n",
      "-- self.device = cuda:0\n",
      "************ End of Model Manager Details ************\n"
     ]
    }
   ],
   "source": [
    "conf.init_logger(logging.INFO, logfile=None)\n",
    "mgr = mm.ModelManager(mode='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:12:31] [INFO] Loading data using SNLI ...\n",
      "[2018-10-28 14:12:31] [INFO] loading raw training data set ...\n",
      "[2018-10-28 14:12:32] [INFO] loading raw training data set ...\n",
      "[2018-10-28 14:12:32] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 14:12:45] [INFO] converting training set to index ...\n",
      "[2018-10-28 14:12:46] [INFO] converting val set to index ...\n",
      "[2018-10-28 14:12:46] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.SNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:12:46] [INFO] \n",
      "*********** Model: modlrinv100decay95rhs100drop25 Details ***********\n",
      "-- self.label = modlrinv100decay95rhs100drop25\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.25\n",
      "-- self.hparams.dropout_fc = 0.25\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 100\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16168204  0.08192482 -0.10181119 ... -0.08808727 -0.01877083\n",
      "  -0.0851997 ]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/modlrinv100decay95rhs100drop25/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 10\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout_rnn = 0.25\n",
      "-- self.output_dict.dropout_fc = 0.25\n",
      "-- self.output_dict.batch_size = 256\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_kernal_size = 3\n",
      "-- self.output_dict.rnn_num_layers = 1\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 50\n",
      "-- self.output_dict.no_imp_look_back = 25\n",
      "-- self.output_dict.decay_lr_no_improv = 0.5\n",
      "-- self.output_dict.es_req_prog = 0.0\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: modlrinv100decay95rhs100drop25 Details ************\n",
      "[2018-10-28 14:12:47] [INFO] New Model initialized: /modlrinv100decay95rhs100drop25, all model output files will be saved here: model_saves/modlrinv100decay95rhs100drop25/\n",
      "[2018-10-28 14:12:47] [INFO] loading checkpoint at model_saves/modlrinv100decay95rhs100drop25/model_best.tar\n",
      "[2018-10-28 14:12:49] [INFO] Successfully loaded checkpoint!\n"
     ]
    }
   ],
   "source": [
    "hparams={\n",
    "    HyperParamKey.LR: 0.01,\n",
    "    HyperParamKey.SCHEDULER_GAMMA: 0.95,\n",
    "    HyperParamKey.BATCH_SIZE: 256,\n",
    "    HyperParamKey.RNN_HIDDEN_SIZE: 100,\n",
    "    HyperParamKey.DROPOUT_FC: 0.25,\n",
    "    HyperParamKey.DROPOUT_RNN: 0.25,\n",
    "    HyperParamKey.NO_IMPROV_LOOK_BACK: 25\n",
    "}\n",
    "mgr.hparams.update(hparams)\n",
    "mgr.new_model(mm.modelRegister.NLIRNN, label='modlrinv100decay95rhs100drop25')\n",
    "mgr.load_model(which_model=LoadingKey.LOAD_BEST)\n",
    "mgr.propogate_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.0, 3.3691309690475464)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgr.model.eval_model(mgr.dataloader.loaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the MNLI Data - Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:17:43] [INFO] \n",
      "*********** Model: fiction Details ***********\n",
      "-- self.label = fiction\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.25\n",
      "-- self.hparams.dropout_fc = 0.25\n",
      "-- self.hparams.batch_size = 32\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 100\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.08798182 -0.1545669   0.00276842 ... -0.07708442  0.06897857\n",
      "  -0.16087203]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/fiction/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 10\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout_rnn = 0.25\n",
      "-- self.output_dict.dropout_fc = 0.25\n",
      "-- self.output_dict.batch_size = 32\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_kernal_size = 3\n",
      "-- self.output_dict.rnn_num_layers = 1\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 50\n",
      "-- self.output_dict.no_imp_look_back = 25\n",
      "-- self.output_dict.decay_lr_no_improv = 0.5\n",
      "-- self.output_dict.es_req_prog = 0.0\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: fiction Details ************\n",
      "[2018-10-28 14:17:43] [INFO] New Model initialized: /fiction, all model output files will be saved here: model_saves/fiction/\n",
      "[2018-10-28 14:17:43] [INFO] loading checkpoint at model_saves/modlrinv100decay95rhs100drop25/model_best.tar\n",
      "[2018-10-28 14:17:44] [INFO] Successfully loaded checkpoint!\n"
     ]
    }
   ],
   "source": [
    "hparams={\n",
    "    HyperParamKey.LR: 0.01,\n",
    "    HyperParamKey.SCHEDULER_GAMMA: 0.95,\n",
    "    HyperParamKey.BATCH_SIZE: 32,\n",
    "    HyperParamKey.RNN_HIDDEN_SIZE: 100,\n",
    "    HyperParamKey.DROPOUT_FC: 0.25,\n",
    "    HyperParamKey.DROPOUT_RNN: 0.25,\n",
    "    HyperParamKey.NO_IMPROV_LOOK_BACK: 25\n",
    "}\n",
    "mgr.hparams.update(hparams)\n",
    "mgr.new_model(mm.modelRegister.NLIRNN, label=genre)\n",
    "mgr.load_model(path_to_model_ovrd='model_saves/modlrinv100decay95rhs100drop25/model_best.tar')\n",
    "mgr.propogate_params()  # needed to send params down to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:17:44] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 14:17:44] [INFO] loading raw training data set ...\n",
      "[2018-10-28 14:17:44] [INFO] loading raw training data set ...\n",
      "[2018-10-28 14:17:44] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 14:17:57] [INFO] converting training set to index ...\n",
      "[2018-10-28 14:17:57] [INFO] converting val set to index ...\n",
      "[2018-10-28 14:17:57] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.MNLI, genre=conf.GENRE_LIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.04330312185297, 32.9636749625206)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgr.model.eval_model(mgr.dataloader.loaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:17:58] [INFO] added 2 to required epochs count. \n",
      "cur epoch=5, required epochs=7\n"
     ]
    }
   ],
   "source": [
    "mgr.model.add_epochs(2, reset_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 14:17:58] [INFO] stepped scheduler to epoch = 6\n",
      "[2018-10-28 14:17:59] [INFO] Ep:6/7, Bt:10/120, VAcc:48.24, VLoss:33.4, TAcc:48.74, TLoss:124.8, LR:0.0077\n",
      "[2018-10-28 14:18:00] [INFO] Ep:6/7, Bt:20/120, VAcc:48.74, VLoss:33.4, TAcc:52.62, TLoss:123.6, LR:0.0077\n",
      "[2018-10-28 14:18:03] [INFO] Ep:6/7, Bt:30/120, VAcc:48.24, VLoss:33.4, TAcc:51.00, TLoss:124.2, LR:0.0077\n",
      "[2018-10-28 14:18:04] [INFO] Ep:6/7, Bt:40/120, VAcc:47.43, VLoss:33.7, TAcc:52.41, TLoss:123.5, LR:0.0077\n",
      "[2018-10-28 14:18:05] [INFO] Ep:6/7, Bt:50/120, VAcc:51.06, VLoss:33.4, TAcc:53.53, TLoss:122.3, LR:0.0077\n",
      "[2018-10-28 14:18:08] [INFO] Ep:6/7, Bt:60/120, VAcc:49.55, VLoss:33.3, TAcc:54.19, TLoss:123.1, LR:0.0077\n",
      "[2018-10-28 14:18:09] [INFO] Ep:6/7, Bt:70/120, VAcc:50.35, VLoss:33.0, TAcc:56.19, TLoss:121.8, LR:0.0077\n",
      "[2018-10-28 14:18:10] [INFO] Ep:6/7, Bt:80/120, VAcc:48.84, VLoss:33.3, TAcc:52.72, TLoss:123.2, LR:0.0077\n",
      "[2018-10-28 14:18:12] [INFO] Ep:6/7, Bt:90/120, VAcc:50.96, VLoss:33.0, TAcc:57.89, TLoss:120.5, LR:0.0077\n",
      "[2018-10-28 14:18:13] [INFO] Ep:6/7, Bt:100/120, VAcc:52.06, VLoss:33.0, TAcc:59.35, TLoss:120.6, LR:0.0077\n",
      "[2018-10-28 14:18:16] [INFO] Ep:6/7, Bt:110/120, VAcc:50.15, VLoss:32.8, TAcc:58.17, TLoss:118.9, LR:0.0077\n",
      "[2018-10-28 14:18:17] [INFO] Ep:6/7, Bt:120/120, VAcc:51.46, VLoss:32.8, TAcc:60.03, TLoss:117.9, LR:0.0077\n",
      "[2018-10-28 14:18:20] [INFO] stepped scheduler to epoch = 7\n",
      "[2018-10-28 14:18:21] [INFO] Ep:7/7, Bt:10/120, VAcc:53.17, VLoss:32.7, TAcc:62.11, TLoss:116.3, LR:0.0074\n",
      "[2018-10-28 14:18:25] [INFO] Ep:7/7, Bt:20/120, VAcc:52.97, VLoss:32.7, TAcc:63.65, TLoss:115.5, LR:0.0074\n",
      "[2018-10-28 14:18:26] [INFO] Ep:7/7, Bt:30/120, VAcc:51.86, VLoss:32.5, TAcc:63.49, TLoss:114.1, LR:0.0074\n",
      "[2018-10-28 14:18:27] [INFO] Ep:7/7, Bt:40/120, VAcc:52.37, VLoss:32.4, TAcc:65.74, TLoss:112.8, LR:0.0074\n",
      "[2018-10-28 14:18:28] [INFO] Ep:7/7, Bt:50/120, VAcc:53.07, VLoss:32.6, TAcc:66.94, TLoss:112.3, LR:0.0074\n",
      "[2018-10-28 14:18:29] [INFO] Ep:7/7, Bt:60/120, VAcc:52.47, VLoss:32.4, TAcc:67.33, TLoss:111.1, LR:0.0074\n",
      "[2018-10-28 14:18:30] [INFO] Ep:7/7, Bt:70/120, VAcc:53.78, VLoss:32.3, TAcc:68.71, TLoss:110.6, LR:0.0074\n",
      "[2018-10-28 14:18:33] [INFO] Ep:7/7, Bt:80/120, VAcc:55.19, VLoss:32.3, TAcc:68.81, TLoss:110.3, LR:0.0074\n",
      "[2018-10-28 14:18:36] [INFO] Ep:7/7, Bt:90/120, VAcc:50.55, VLoss:32.8, TAcc:66.96, TLoss:112.6, LR:0.0074\n",
      "[2018-10-28 14:18:37] [INFO] Ep:7/7, Bt:100/120, VAcc:52.37, VLoss:32.7, TAcc:71.21, TLoss:110.2, LR:0.0074\n",
      "[2018-10-28 14:18:38] [INFO] Ep:7/7, Bt:110/120, VAcc:53.37, VLoss:33.0, TAcc:68.79, TLoss:112.5, LR:0.0074\n",
      "[2018-10-28 14:18:39] [INFO] Ep:7/7, Bt:120/120, VAcc:49.75, VLoss:33.0, TAcc:69.83, TLoss:110.5, LR:0.0074\n",
      "[2018-10-28 14:18:44] [INFO] training completed, results collected ...\n"
     ]
    }
   ],
   "source": [
    "mgr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
