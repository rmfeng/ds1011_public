{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import basic_conf as conf\n",
    "from libs import ModelManager as mm\n",
    "from config.constants import HyperParamKey, LoadingKey\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 13:04:38] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-28 13:04:38] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "NLICNN\n",
      "========================\n",
      "[2018-10-28 13:04:38] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "MNLI\n",
      "========================\n",
      "[2018-10-28 13:04:38] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7fe8f32069d8>\n",
      "-- self.device = cuda:0\n",
      "************ End of Model Manager Details ************\n"
     ]
    }
   ],
   "source": [
    "conf.init_logger(logging.INFO, logfile=None)\n",
    "mgr = mm.ModelManager(mode='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the RNN Model and Validating its Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 12:56:25] [INFO] Loading data using SNLI ...\n",
      "[2018-10-28 12:56:25] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:56:25] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:56:25] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:56:38] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:56:39] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:56:39] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.SNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 12:56:39] [INFO] \n",
      "*********** Model: modlrinv100decay95rhs100drop25 Details ***********\n",
      "-- self.label = modlrinv100decay95rhs100drop25\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.25\n",
      "-- self.hparams.dropout_fc = 0.25\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 100\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.09\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.08600217  0.29350724  0.01025207 ...  0.14266631 -0.00119153\n",
      "   0.21694557]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/modlrinv100decay95rhs100drop25/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 10\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout_rnn = 0.25\n",
      "-- self.output_dict.dropout_fc = 0.25\n",
      "-- self.output_dict.batch_size = 256\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_hidden_size = 100\n",
      "-- self.output_dict.cnn_kernal_size = 3\n",
      "-- self.output_dict.rnn_num_layers = 1\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 50\n",
      "-- self.output_dict.no_imp_look_back = 25\n",
      "-- self.output_dict.decay_lr_no_improv = 0.5\n",
      "-- self.output_dict.es_req_prog = 0.0\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.09\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: modlrinv100decay95rhs100drop25 Details ************\n",
      "[2018-10-28 12:56:41] [INFO] New Model initialized: /modlrinv100decay95rhs100drop25, all model output files will be saved here: model_saves/modlrinv100decay95rhs100drop25/\n",
      "[2018-10-28 12:56:41] [INFO] loading checkpoint at model_saves/modlrinv100decay95rhs100drop25/model_best.tar\n",
      "[2018-10-28 12:56:42] [INFO] Successfully loaded checkpoint!\n"
     ]
    }
   ],
   "source": [
    "hparams={\n",
    "    HyperParamKey.LR: 0.01,\n",
    "    HyperParamKey.SCHEDULER_GAMMA: 0.09,\n",
    "    HyperParamKey.RNN_HIDDEN_SIZE: 100,\n",
    "    HyperParamKey.DROPOUT_FC: 0.25,\n",
    "    HyperParamKey.DROPOUT_RNN: 0.25,\n",
    "}\n",
    "mgr.hparams.update(hparams)\n",
    "mgr.new_model(mm.modelRegister.NLIRNN, label='modlrinv100decay95rhs100drop25')\n",
    "mgr.load_model(which_model=LoadingKey.LOAD_BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.4, 3.5265949964523315)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgr.model.eval_model(mgr.dataloader.loaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Acc on a MNLI Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 12:57:28] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 12:57:28] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:28] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:28] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:57:41] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:57:42] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:57:42] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 12:57:42] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 12:57:42] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:42] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:42] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:57:55] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:57:55] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:57:55] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 12:57:56] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 12:57:56] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:56] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:57:56] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:58:09] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:58:09] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:58:09] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 12:58:09] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 12:58:09] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:58:09] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:58:09] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:58:22] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:58:22] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:58:22] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 12:58:22] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 12:58:22] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:58:22] [INFO] loading raw training data set ...\n",
      "[2018-10-28 12:58:22] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 12:58:36] [INFO] converting training set to index ...\n",
      "[2018-10-28 12:58:36] [INFO] converting val set to index ...\n",
      "[2018-10-28 12:58:36] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fiction': 44.52261306532663,\n",
       " 'telephone': 46.865671641791046,\n",
       " 'slate': 44.81037924151697,\n",
       " 'government': 44.98031496062992,\n",
       " 'travel': 45.824847250509166}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_res = {}\n",
    "for genre in conf.GENRE_LIST:\n",
    "    mgr.load_data(mm.loaderRegister.MNLI, genre=genre)\n",
    "    acc = mgr.model.eval_model(mgr.dataloader.loaders['val'])[0]\n",
    "    rnn_res[genre] = acc\n",
    "    \n",
    "rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 13:06:31] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-28 13:06:31] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "NLICNN\n",
      "========================\n",
      "[2018-10-28 13:06:31] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "MNLI\n",
      "========================\n",
      "[2018-10-28 13:06:31] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.cnn_hidden_size = 100\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7fe8f32069d8>\n",
      "-- self.device = cuda:0\n",
      "************ End of Model Manager Details ************\n",
      "[2018-10-28 13:06:31] [INFO] Loading data using SNLI ...\n",
      "[2018-10-28 13:06:31] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:06:31] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:06:31] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:06:44] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:06:45] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:06:45] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "# resetting\n",
    "mgr = mm.ModelManager(mode='notebook')\n",
    "mgr.load_data(mm.loaderRegister.SNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 13:06:45] [INFO] \n",
      "*********** Model: cnnlrinv1000hs200kern3 Details ***********\n",
      "-- self.label = cnnlrinv1000hs200kern3\n",
      "-- self.hparams.num_epochs = 10\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 256\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.cnn_hidden_size = 200\n",
      "-- self.hparams.cnn_kernal_size = 3\n",
      "-- self.hparams.rnn_num_layers = 1\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 50\n",
      "-- self.hparams.no_imp_look_back = 25\n",
      "-- self.hparams.decay_lr_no_improv = 0.5\n",
      "-- self.hparams.es_req_prog = 0.0\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11739849 -0.06893712  0.23082722 ...  0.08550936  0.03048519\n",
      "   0.07482267]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.snli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.snli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.mnli_train_path = data/nli/mnli_train.tsv\n",
      "-- self.cparams.mnli_val_path = data/nli/mnli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/cnnlrinv1000hs200kern3/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 10\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout_rnn = 0.5\n",
      "-- self.output_dict.dropout_fc = 0.5\n",
      "-- self.output_dict.batch_size = 256\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 50\n",
      "-- self.output_dict.cnn_hidden_size = 200\n",
      "-- self.output_dict.cnn_kernal_size = 3\n",
      "-- self.output_dict.rnn_num_layers = 1\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 50\n",
      "-- self.output_dict.no_imp_look_back = 25\n",
      "-- self.output_dict.decay_lr_no_improv = 0.5\n",
      "-- self.output_dict.es_req_prog = 0.0\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: cnnlrinv1000hs200kern3 Details ************\n",
      "[2018-10-28 13:06:46] [INFO] New Model initialized: /cnnlrinv1000hs200kern3, all model output files will be saved here: model_saves/cnnlrinv1000hs200kern3/\n",
      "[2018-10-28 13:06:46] [INFO] loading checkpoint at model_saves/cnnlrinv1000hs200kern3/model_best.tar\n",
      "[2018-10-28 13:06:47] [INFO] Successfully loaded checkpoint!\n"
     ]
    }
   ],
   "source": [
    "hparams={\n",
    "    HyperParamKey.LR: 0.01,\n",
    "    HyperParamKey.CNN_HIDDEN_SIZE: 200,\n",
    "    HyperParamKey.CNN_KERNAL_SIZE: 3\n",
    "}\n",
    "mgr.hparams.update(hparams)\n",
    "mgr.new_model(mm.modelRegister.NLICNN, label='cnnlrinv1000hs200kern3')\n",
    "mgr.load_model(which_model=LoadingKey.LOAD_BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.3, 3.550516128540039)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgr.model.eval_model(mgr.dataloader.loaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the MNLI Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-28 13:06:47] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 13:06:47] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:06:47] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:06:47] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:07:00] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:07:00] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:07:00] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 13:07:00] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 13:07:00] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:01] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:01] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:07:14] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:07:14] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:07:14] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 13:07:14] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 13:07:14] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:14] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:14] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:07:27] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:07:27] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:07:27] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 13:07:27] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 13:07:27] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:27] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:27] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:07:40] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:07:40] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:07:40] [INFO] piping data into pytorch DataLoaders ...\n",
      "[2018-10-28 13:07:40] [INFO] Loading data using MNLI ...\n",
      "[2018-10-28 13:07:40] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:40] [INFO] loading raw training data set ...\n",
      "[2018-10-28 13:07:40] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-28 13:07:53] [INFO] converting training set to index ...\n",
      "[2018-10-28 13:07:53] [INFO] converting val set to index ...\n",
      "[2018-10-28 13:07:53] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fiction': 45.82074521651561,\n",
       " 'telephone': 46.46766169154229,\n",
       " 'slate': 43.11377245508982,\n",
       " 'government': 43.503937007874015,\n",
       " 'travel': 46.537678207739305}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_res = {}\n",
    "for genre in conf.GENRE_LIST:\n",
    "    mgr.load_data(mm.loaderRegister.MNLI, genre=genre)\n",
    "    acc = mgr.model.eval_model(mgr.dataloader.loaders['val'])[0]\n",
    "    cnn_res[genre] = acc\n",
    "    \n",
    "cnn_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
