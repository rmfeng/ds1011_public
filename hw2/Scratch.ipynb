{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from libs import ModelManager as mm\n",
    "from config import basic_conf as conf\n",
    "import logging\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 11:51:14] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-27 11:51:14] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "========================\n",
      "[2018-10-27 11:51:14] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "========================\n",
      "[2018-10-27 11:51:14] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 1\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout = 0.3\n",
      "-- self.hparams.batch_size = 4\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.rnn_num_layers = 2\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7f221a9c7b70>\n",
      "************ End of Model Manager Details ************\n"
     ]
    }
   ],
   "source": [
    "conf.init_logger(logging.INFO, logfile=None)\n",
    "mgr = mm.ModelManager(mode='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 11:51:14] [INFO] Loading data using SNLI ...\n",
      "[2018-10-27 11:51:14] [INFO] loading raw training data set ...\n",
      "[2018-10-27 11:51:14] [INFO] loading raw training data set ...\n",
      "[2018-10-27 11:51:14] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-27 11:51:27] [INFO] converting training set to index ...\n",
      "[2018-10-27 11:51:28] [INFO] converting val set to index ...\n",
      "[2018-10-27 11:51:28] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.SNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 11:51:28] [INFO] \n",
      "*********** Model: scratch Details ***********\n",
      "-- self.label = scratch\n",
      "-- self.hparams.num_epochs = 1\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout = 0.3\n",
      "-- self.hparams.batch_size = 4\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.rnn_num_layers = 2\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.32235227  0.33276341 -0.0434354  ...  0.23271052 -0.31735096\n",
      "  -0.00992109]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/scratch/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 1\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout = 0.3\n",
      "-- self.output_dict.batch_size = 4\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 50\n",
      "-- self.output_dict.rnn_num_layers = 2\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 10\n",
      "-- self.output_dict.es_req_prog = 0.01\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.32235227  0.33276341 -0.0434354  ...  0.23271052 -0.31735096\n",
      "  -0.00992109]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: scratch Details ************\n",
      "[2018-10-27 11:51:29] [INFO] New Model initialized: /scratch, all model output files will be saved here: model_saves/scratch/\n"
     ]
    }
   ],
   "source": [
    "mgr.new_model(mm.modelRegister.NLIRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mgr.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(100002, 300, padding_idx=0)\n",
       "  (rnn): GRU(300, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = mgr.dataloader.loaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent1_batch, sent2_batch, len1_batch, len2_batch, label_batch in l:\n",
    "    print(len1_batch)\n",
    "    \n",
    "    outputs = model(sent1_batch,\n",
    "                    sent2_batch,\n",
    "                    len1_batch,\n",
    "                    len2_batch) \n",
    "    \n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
