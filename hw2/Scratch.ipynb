{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from libs import ModelManager as mm\n",
    "from config import basic_conf as conf\n",
    "import logging\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 13:13:29] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-27 13:13:29] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "========================\n",
      "[2018-10-27 13:13:29] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "========================\n",
      "[2018-10-27 13:13:29] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 1\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout = 0.5\n",
      "-- self.hparams.batch_size = 4\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.rnn_num_layers = 2\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7fdb8e5c4b70>\n",
      "-- self.device = cpu\n",
      "************ End of Model Manager Details ************\n"
     ]
    }
   ],
   "source": [
    "conf.init_logger(logging.INFO, logfile=None)\n",
    "mgr = mm.ModelManager(mode='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 13:13:29] [INFO] Loading data using SNLI ...\n",
      "[2018-10-27 13:13:29] [INFO] loading raw training data set ...\n",
      "[2018-10-27 13:13:29] [INFO] loading raw training data set ...\n",
      "[2018-10-27 13:13:29] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-27 13:13:42] [INFO] converting training set to index ...\n",
      "[2018-10-27 13:13:42] [INFO] converting val set to index ...\n",
      "[2018-10-27 13:13:42] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.SNLI, train_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 13:13:42] [INFO] \n",
      "*********** Model: scratch Details ***********\n",
      "-- self.label = scratch\n",
      "-- self.hparams.num_epochs = 1\n",
      "-- self.hparams.lr = 0.01\n",
      "-- self.hparams.voc_size = 100000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout = 0.5\n",
      "-- self.hparams.batch_size = 4\n",
      "-- self.hparams.fc_hidden_size = 100\n",
      "-- self.hparams.rnn_hidden_size = 50\n",
      "-- self.hparams.rnn_num_layers = 2\n",
      "-- self.hparams.check_early_stop = True\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 100002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01750315  0.25247643 -0.09221568 ...  0.09094705  0.17975928\n",
      "   0.00641187]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/scratch/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 1\n",
      "-- self.output_dict.lr = 0.01\n",
      "-- self.output_dict.voc_size = 100000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout = 0.5\n",
      "-- self.output_dict.batch_size = 4\n",
      "-- self.output_dict.fc_hidden_size = 100\n",
      "-- self.output_dict.rnn_hidden_size = 50\n",
      "-- self.output_dict.rnn_num_layers = 2\n",
      "-- self.output_dict.check_early_stop = True\n",
      "-- self.output_dict.es_look_back = 10\n",
      "-- self.output_dict.es_req_prog = 0.01\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 100002\n",
      "-- self.output_dict.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01750315  0.25247643 -0.09221568 ...  0.09094705  0.17975928\n",
      "   0.00641187]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [ 0.1364     -0.0823      0.0268     ...  0.0146     -0.1281\n",
      "   0.1004    ]\n",
      " [ 0.3732      0.0413      0.179      ... -0.0461     -0.0787\n",
      "  -0.0635    ]\n",
      " [-0.0405     -0.0471      0.1363     ...  0.2969     -0.223\n",
      "  -0.0133    ]]\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: scratch Details ************\n",
      "[2018-10-27 13:13:42] [INFO] New Model initialized: /scratch, all model output files will be saved here: model_saves/scratch/\n"
     ]
    }
   ],
   "source": [
    "mgr.new_model(mm.modelRegister.NLIRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mgr.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(100002, 300)\n",
       "  (rnn): GRU(300, 50, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_data = mgr.dataloader.data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = l_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9, 802, 1830, 8, 9, 6265, 7167, 4388, 17, 9, 12229, 5335, 9, 563, 6, 358, 4],\n",
       " [9, 802, 1830, 2255, 3, 13985, 94, 3, 358, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di.token_idx_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = mgr.dataloader.loaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17])\n",
      "tensor([[    9,   802,  1830,     8,     9,  6265,  7167,  4388,    17,     9,\n",
      "         12229,  5335,     9,   563,     6,   358,     4]])\n"
     ]
    }
   ],
   "source": [
    "for sent1_batch, sent2_batch, len1_batch, len2_batch, label_batch in l:\n",
    "    print(len1_batch)\n",
    "    print(sent1_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0762, -0.4489,  0.5261, -1.0677,  0.8781],\n",
       "         [ 0.7237,  2.6297, -1.5563,  1.1433, -0.4791],\n",
       "         [-1.0312,  0.3659,  0.0245, -0.7548, -0.1548],\n",
       "         [ 0.2657,  0.2099,  0.0686,  0.5378,  1.0993]],\n",
       "\n",
       "        [[-1.0358,  0.4978, -1.2232, -0.4774,  0.3099],\n",
       "         [-2.2463,  0.0905,  0.3335,  1.2132,  1.1002],\n",
       "         [ 0.3427, -0.5845, -0.5027,  1.1077, -0.4983],\n",
       "         [ 0.5663,  0.6057, -0.1243,  0.3221,  0.1894]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor for dim (4, 4, 10)\n",
    "tsr = torch.FloatTensor(torch.randn(2, 4, 5))\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0762, -0.4489,  0.5261, -1.0677,  0.8781],\n",
       "         [ 0.7237,  2.6297, -1.5563,  1.1433, -0.4791],\n",
       "         [-1.0312,  0.3659,  0.0245, -0.7548, -0.1548],\n",
       "         [ 0.2657,  0.2099,  0.0686,  0.5378,  1.0993]],\n",
       "\n",
       "        [[-1.0358,  0.4978, -1.2232, -0.4774,  0.3099],\n",
       "         [-2.2463,  0.0905,  0.3335,  1.2132,  1.1002],\n",
       "         [ 0.3427, -0.5845, -0.5027,  1.1077, -0.4983],\n",
       "         [ 0.5663,  0.6057, -0.1243,  0.3221,  0.1894]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr_tuple = tuple(tsr[x] for x in range(tsr.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0762, -0.4489,  0.5261, -1.0677,  0.8781, -1.0358,  0.4978, -1.2232,\n",
       "         -0.4774,  0.3099],\n",
       "        [ 0.7237,  2.6297, -1.5563,  1.1433, -0.4791, -2.2463,  0.0905,  0.3335,\n",
       "          1.2132,  1.1002],\n",
       "        [-1.0312,  0.3659,  0.0245, -0.7548, -0.1548,  0.3427, -0.5845, -0.5027,\n",
       "          1.1077, -0.4983],\n",
       "        [ 0.2657,  0.2099,  0.0686,  0.5378,  1.0993,  0.5663,  0.6057, -0.1243,\n",
       "          0.3221,  0.1894]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(tsr_tuple, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "_input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(_input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = [4, 4, 50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
