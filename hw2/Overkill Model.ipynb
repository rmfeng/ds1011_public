{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import basic_conf as conf\n",
    "conf.DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from libs import ModelManager as mm\n",
    "from config.constants import HyperParamKey\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 20:46:40] [INFO] Initializing Model Manager, version 0.4.0 ...\n",
      "[2018-10-27 20:46:40] [INFO] \n",
      "=== Models Available ===\n",
      "BagOfWords\n",
      "NLIRNN\n",
      "========================\n",
      "[2018-10-27 20:46:40] [INFO] \n",
      "=== Loaders Available ===\n",
      "IMDB\n",
      "SNLI\n",
      "========================\n",
      "[2018-10-27 20:46:40] [INFO] \n",
      "*********** Model Manager Details ***********\n",
      "-- self.hparams.num_epochs = 20\n",
      "-- self.hparams.lr = 0.003\n",
      "-- self.hparams.voc_size = 500000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 512\n",
      "-- self.hparams.fc_hidden_size = 200\n",
      "-- self.hparams.rnn_hidden_size = 200\n",
      "-- self.hparams.rnn_num_layers = 8\n",
      "-- self.hparams.check_early_stop = False\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.decay_lr_no_improv = 0.25\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.lparams = None\n",
      "-- self.model = None\n",
      "-- self.dataloader = None\n",
      "-- self.results = []\n",
      "-- self.mode = notebook\n",
      "-- self.tqdm = <function tqdm_notebook at 0x7f4fe2daa048>\n",
      "-- self.device = cuda:1\n",
      "************ End of Model Manager Details ************\n"
     ]
    }
   ],
   "source": [
    "conf.init_logger(logging.INFO, logfile=None)\n",
    "hparams = {\n",
    "    HyperParamKey.LR: 0.003,\n",
    "    HyperParamKey.NUM_EPOCH: 20,\n",
    "    HyperParamKey.BATCH_SIZE: 512,\n",
    "    HyperParamKey.CHECK_EARLY_STOP: False,\n",
    "    HyperParamKey.FC_HIDDEN_SIZE: 200,\n",
    "    HyperParamKey.RNN_HIDDEN_SIZE: 200,\n",
    "    HyperParamKey.VOC_SIZE: 500000,\n",
    "    HyperParamKey.RNN_NUM_LAYERS: 8,\n",
    "}\n",
    "mgr = mm.ModelManager(hparams=hparams, mode='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 20:46:47] [INFO] Loading data using SNLI ...\n",
      "[2018-10-27 20:46:47] [INFO] loading raw training data set ...\n",
      "[2018-10-27 20:46:48] [INFO] loading raw training data set ...\n",
      "[2018-10-27 20:46:48] [INFO] loading pre-trained word vectors, building vocab ...\n",
      "[2018-10-27 20:47:52] [INFO] converting training set to index ...\n",
      "[2018-10-27 20:47:53] [INFO] converting val set to index ...\n",
      "[2018-10-27 20:47:53] [INFO] piping data into pytorch DataLoaders ...\n"
     ]
    }
   ],
   "source": [
    "mgr.load_data(mm.loaderRegister.SNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-27 20:48:40] [INFO] \n",
      "*********** Model: scratch Details ***********\n",
      "-- self.label = scratch\n",
      "-- self.hparams.num_epochs = 20\n",
      "-- self.hparams.lr = 0.003\n",
      "-- self.hparams.voc_size = 500000\n",
      "-- self.hparams.train_loop_check_freq = 10\n",
      "-- self.hparams.dropout_rnn = 0.5\n",
      "-- self.hparams.dropout_fc = 0.5\n",
      "-- self.hparams.batch_size = 512\n",
      "-- self.hparams.fc_hidden_size = 200\n",
      "-- self.hparams.rnn_hidden_size = 200\n",
      "-- self.hparams.rnn_num_layers = 8\n",
      "-- self.hparams.check_early_stop = False\n",
      "-- self.hparams.es_look_back = 10\n",
      "-- self.hparams.decay_lr_no_improv = 0.25\n",
      "-- self.hparams.es_req_prog = 0.01\n",
      "-- self.hparams.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.hparams.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.hparams.scheduler_gamma = 0.95\n",
      "-- self.hparams.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.lparams.act_vocab_size = 500002\n",
      "-- self.lparams.pre_trained_vecs = [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.056569    0.03537296  0.05610529 ...  0.08404023  0.2048222\n",
      "   0.03724429]\n",
      " [ 0.1073      0.0089      0.0006     ...  0.005       0.1173\n",
      "  -0.04      ]\n",
      " ...\n",
      " [-0.0338      0.1384      0.0275     ... -0.1685      0.0553\n",
      "   0.0174    ]\n",
      " [-0.0894     -0.0691     -0.0506     ... -0.1319      0.0435\n",
      "   0.1492    ]\n",
      " [ 0.0376     -0.0404      0.008      ... -0.0592      0.0038\n",
      "   0.0191    ]]\n",
      "-- self.lparams.embedding_dim = 300\n",
      "-- self.lparams.num_classes = 3\n",
      "-- self.cparams.save_best_model = True\n",
      "-- self.cparams.save_each_epoch = True\n",
      "-- self.cparams.ignore_params = ['pre_trained_vecs']\n",
      "-- self.cparams.nli_train_path = data/nli/snli_train.tsv\n",
      "-- self.cparams.nli_val_path = data/nli/snli_val.tsv\n",
      "-- self.cparams.pretrained_path = data/nli/wiki-news-300d-1M.vec\n",
      "-- self.cparams.model_saves = model_saves/\n",
      "-- self.cparams.model_path = model_saves/scratch/\n",
      "-- self.cur_epoch = 0\n",
      "-- self.model = None\n",
      "-- self.optim = None\n",
      "-- self.scheduler = None\n",
      "-- self.iter_curves.train_acc = []\n",
      "-- self.iter_curves.train_loss = []\n",
      "-- self.iter_curves.val_acc = []\n",
      "-- self.iter_curves.val_loss = []\n",
      "-- self.epoch_curves.train_acc = []\n",
      "-- self.epoch_curves.train_loss = []\n",
      "-- self.epoch_curves.val_acc = []\n",
      "-- self.epoch_curves.val_loss = []\n",
      "-- self.output_dict.num_epochs = 20\n",
      "-- self.output_dict.lr = 0.003\n",
      "-- self.output_dict.voc_size = 500000\n",
      "-- self.output_dict.train_loop_check_freq = 10\n",
      "-- self.output_dict.dropout_rnn = 0.5\n",
      "-- self.output_dict.dropout_fc = 0.5\n",
      "-- self.output_dict.batch_size = 512\n",
      "-- self.output_dict.fc_hidden_size = 200\n",
      "-- self.output_dict.rnn_hidden_size = 200\n",
      "-- self.output_dict.rnn_num_layers = 8\n",
      "-- self.output_dict.check_early_stop = False\n",
      "-- self.output_dict.es_look_back = 10\n",
      "-- self.output_dict.decay_lr_no_improv = 0.25\n",
      "-- self.output_dict.es_req_prog = 0.01\n",
      "-- self.output_dict.optim_enc = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.optim_dec = <class 'torch.optim.adam.Adam'>\n",
      "-- self.output_dict.scheduler = <class 'torch.optim.lr_scheduler.ExponentialLR'>\n",
      "-- self.output_dict.scheduler_gamma = 0.95\n",
      "-- self.output_dict.criterion = <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "-- self.output_dict.act_vocab_size = 500002\n",
      "-- self.output_dict.embedding_dim = 300\n",
      "-- self.output_dict.num_classes = 3\n",
      "************ End of Model: scratch Details ************\n",
      "[2018-10-27 20:48:43] [INFO] New Model initialized: /scratch, all model output files will be saved here: model_saves/scratch/\n"
     ]
    }
   ],
   "source": [
    "mgr.new_model(mm.modelRegister.NLIRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
