{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from importlib import reload\n",
    "\n",
    "import IMDBDatum as imdb_data\n",
    "import ngrams\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "TRAINING_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "NGRAM_SIZE = 2 # (1, 2, 3, 4)\n",
    "VOC_SIZE = 10000 # takes top n word from the vocab\n",
    "EMBEDDING_DIM = 100 # dimension size for the ngram embeddings\n",
    "NGRAM_MODE = 'naive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "data_dir = r'./data/aclImdb/'\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "TRAIN_SIZE = 20000\n",
    "VALIDATION_SIZE = 5000\n",
    "TEST_SIZE = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\n",
      "|-- \u001b[01;34m__pycache__\u001b[00m\n",
      "`-- \u001b[01;34mdata\u001b[00m\n",
      "    `-- \u001b[01;34maclImdb\u001b[00m\n",
      "        |-- \u001b[01;34mtest\u001b[00m\n",
      "        |   |-- \u001b[01;34mneg\u001b[00m\n",
      "        |   `-- \u001b[01;34mpos\u001b[00m\n",
      "        `-- \u001b[01;34mtrain\u001b[00m\n",
      "            |-- \u001b[01;34mneg\u001b[00m\n",
      "            |-- \u001b[01;34mpos\u001b[00m\n",
      "            `-- \u001b[01;34munsup\u001b[00m\n",
      "\n",
      "10 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset - should take less than 1 min\n",
    "train_set = imdb_data.construct_dataset(train_dir, TRAIN_SIZE)\n",
    "validation_set = imdb_data.construct_dataset(train_dir, VALIDATION_SIZE, offset=int(TRAIN_SIZE/2))\n",
    "test_set = imdb_data.construct_dataset(test_dir, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch - trying the ngrams code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58dc4a68143433f81fff8e25102cdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = train_set\n",
    "n = NGRAM_SIZE\n",
    "TRIAL_SIZE = 100\n",
    "\n",
    "for i in tqdm(range(0, TRIAL_SIZE)):\n",
    "    text_datum = dataset[i].raw_text\n",
    "    ngrams, tokens = extract_ngram_from_text(text_datum, n)\n",
    "    dataset[i].set_ngram(ngrams)\n",
    "    dataset[i].set_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "PAD_TOKEN, PAD_IDX = '<pad>', 0\n",
    "UNK_TOKEN, UNK_IDX = '<unk>', 1\n",
    "\n",
    "\n",
    "def tokenize(sent, remove_stopwords=True, remove_punc=True, mode='spacy'):\n",
    "    \"\"\"\n",
    "    basic tokenizer method from spacy\n",
    "    :param sent: input sentence\n",
    "    :param remove_stopwords: whether to remove stopwords\n",
    "    :param remove_punc: whether to remove punctuation\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    if mode == 'spacy':\n",
    "        tokens = tokenizer(sent)\n",
    "        tokens = [token.text for token in tokens]\n",
    "        \n",
    "    elif mode == 'naive':\n",
    "        tokens = sent.split(\" \")\n",
    "        \n",
    "    if remove_stopwords:  # only removed if small cap\n",
    "        tokens = [token for token in tokens if token not in stop_words.ENGLISH_STOP_WORDS]\n",
    "        \n",
    "    if remove_punc:\n",
    "        tokens = [token.lower() for token in tokens if (token not in punctuations)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # returns lower case, scrubbed tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_ngram_from_text(text, n, remove_stopwords=True, remove_punc=True, mode='spacy'):\n",
    "    \"\"\"\n",
    "    Function that retrieves all n-grams from the input string\n",
    "    @param text: raw string\n",
    "    @param n: integer that tells the model to retrieve all k-gram where k<=n\n",
    "    @param remove_stopwords: whether or not to remove stopwords from lib\n",
    "    @param remove_punc: whether or not to remove punctuation from lib\n",
    "    @return ngram_counter: a counter that maps n-gram to its frequency\n",
    "    @return tokens: a list of parsed ngrams\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text, remove_stopwords=remove_stopwords, remove_punc=remove_punc, mode=mode)\n",
    "    all_ngrams = []\n",
    "    for i in range(0, len(tokens) - n):\n",
    "        for j in range(1, n + 1):\n",
    "            all_ngrams.append(get_n_gram_at_position_i(j, i, tokens))\n",
    "    ngram_counter = Counter(all_ngrams)\n",
    "    return ngram_counter, all_ngrams\n",
    "    \n",
    "    \n",
    "def construct_ngram_indexer(ngram_counter_list, topk):\n",
    "    \"\"\"\n",
    "    Function that selects the most common topk ngrams\n",
    "    index 0 reserved for <pad>\n",
    "    index 1 reserved for <unk>\n",
    "    @param ngram_counter_list: list of counters\n",
    "    @param topk, int: # of words to keep in the vocabulary - not counting pad/unk\n",
    "    @return ngram2idx: a dictionary that maps ngram to an unique index\n",
    "    \"\"\"\n",
    "    rt_dict = {PAD_TOKEN: PAD_IDX, UNK_TOKEN: UNK_IDX}\n",
    "    i = 2  # the index to start the rest of the tokens\n",
    "    final_count = Counter()\n",
    "\n",
    "    for elem in tqdm(ngram_counter_list):\n",
    "        for key, value in elem.items():\n",
    "            final_count[key] += value\n",
    "\n",
    "    for key in tqdm(dict(final_count.most_common(topk))):\n",
    "        rt_dict[key] = i\n",
    "        i += 1\n",
    "\n",
    "    return rt_dict, final_count  # length topk + 2\n",
    "\n",
    "\n",
    "def get_n_gram_at_position_i(n, i, tokens):\n",
    "    \"\"\"\n",
    "    provided a list of tokens, gets the ngram starting at position i (0 indexed)\n",
    "    :param n: ngram size\n",
    "    :param i: ith position\n",
    "    :param tokens: full list of tokens\n",
    "    :return: tuple representing ngram\n",
    "    \"\"\"\n",
    "    out_list = []\n",
    "    if n == 1:\n",
    "        return tokens[i]\n",
    "    else:\n",
    "        for j in range(i, i + n):\n",
    "            out_list.append(tokens[j])\n",
    "    return tuple(out_list)\n",
    "\n",
    "\n",
    "def token_to_index(tokens, ngram_indexer):\n",
    "    \"\"\"\n",
    "    Function that transform a list of tokens to a list of token index.\n",
    "    index 0 reserved for <pad>\n",
    "    index 1 reserved for <unk>\n",
    "    @param tokens: list of ngram\n",
    "    @param ngram_indexer: a dictionary that maps ngram to an unique index\n",
    "    \"\"\"\n",
    "    return [ngram_indexer[token] if token in ngram_indexer else UNK_IDX for token in tokens]\n",
    "\n",
    "\n",
    "def process_text_dataset(dataset, \n",
    "                         n, \n",
    "                         topk=None, \n",
    "                         ngram_indexer=None, \n",
    "                         remove_stopwords=True, \n",
    "                         remove_punc=True, \n",
    "                         mode='spacy'):\n",
    "    \"\"\"\n",
    "    Top level function that encodes each datum into a list of ngram indices\n",
    "    @param dataset: list of IMDBDatum\n",
    "    @param n: n in \"n-gram\"\n",
    "    @param topk: #\n",
    "    @param ngram_indexer: a dictionary that maps ngram to an unique index\n",
    "    \"\"\"\n",
    "    ngram_counter = None\n",
    "    # extract n-gram\n",
    "    print(\"extracting ngrams ...\")\n",
    "    for i in tnrange(len(dataset), desc='extract ngrams'):\n",
    "        text_datum = dataset[i].raw_text\n",
    "        ngrams, tokens = extract_ngram_from_text(text_datum, n, remove_stopwords, remove_punc, mode)\n",
    "        dataset[i].set_ngram(ngrams)\n",
    "        dataset[i].set_tokens(tokens)\n",
    "    # select top k ngram\n",
    "    if ngram_indexer is None:\n",
    "        print(\"constructing ngram_indexer ...\")\n",
    "        ngram_indexer, ngram_counter = construct_ngram_indexer([datum.ngram for datum in dataset], topk)\n",
    "    else:\n",
    "        print(\"already have a passed ngram_indexer ...\")\n",
    "    # vectorize each datum\n",
    "    print(\"setting each dataset's token indexes\")\n",
    "    for i in tnrange(len(dataset), desc='token to index'):\n",
    "        dataset[i].set_token_idx(token_to_index(dataset[i].tokens, ngram_indexer))\n",
    "    return dataset, ngram_indexer, ngram_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting ngrams ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f405fef3e84e45a65875e59a75544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='extract ngrams', max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing ngram_indexer ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4936340facc84e6496fa7c1d0da12e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5aefaf5a84546ac467b53df4915b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting each dataset's token indexes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af9d3e05808493cbf3874ffffb11129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='token to index', max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting ngrams ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7e3303bb574656b73deea30fbd735f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='extract ngrams', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have a passed ngram_indexer ...\n",
      "setting each dataset's token indexes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c17860d58a427bb71401a6a4f084ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='token to index', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting ngrams ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ee61d7d63b46e4982a51a21225531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='extract ngrams', max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have a passed ngram_indexer ...\n",
      "setting each dataset's token indexes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6740e321198e4db39a22d8933cbf3504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='token to index', max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reload(ngrams)\n",
    "# Note that we are using the train_ngram_indexer to index validation and test dataset. Why? \n",
    "train_data, train_ngram_indexer, ngram_counter = process_text_dataset(train_set, NGRAM_SIZE, VOC_SIZE, \n",
    "                                                       mode=NGRAM_MODE)\n",
    "\n",
    "validation_data, _, _ = process_text_dataset(validation_set, NGRAM_SIZE, ngram_indexer=train_ngram_indexer,\n",
    "                                          mode=NGRAM_MODE)\n",
    "\n",
    "test_data, _, _ = process_text_dataset(test_set, NGRAM_SIZE, ngram_indexer=train_ngram_indexer, \n",
    "                                    mode=NGRAM_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
