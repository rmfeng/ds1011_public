{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ModelManager as mm_mod\n",
    "import config_defaults as cd\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model and Data pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "reload(mm_mod)\n",
    "reload(cd)\n",
    "logger = logging.getLogger('__main__')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting the ngrams for n = 1, 2, 3, 4 with both naive and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_list = (1, 2, 3, 4)\\nmode_list = (\\'naive\\', \\'spacy\\')\\n\\nfor n in n_list:\\n    for mode in mode_list:\\n        print(\"extracting n-grams for: n=%s, mode=%s\" % (n, mode))\\n        param_overrides = {\\'NGRAM_MODE\\': mode,\\n                           \\'NGRAM_SIZE\\': n}\\n        mm = mm_mod.ModelManager(hparams=param_overrides)\\n        mm.load_data()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.2 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_list = (1, 2, 3, 4)\n",
    "mode_list = ('naive', 'spacy')\n",
    "\n",
    "for n in n_list:\n",
    "    for mode in mode_list:\n",
    "        print(\"extracting n-grams for: n=%s, mode=%s\" % (n, mode))\n",
    "        param_overrides = {'NGRAM_MODE': mode,\n",
    "                           'NGRAM_SIZE': n}\n",
    "        mm = mm_mod.ModelManager(hparams=param_overrides)\n",
    "        mm.load_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the annealing of LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-06 16:11:39,701 __main__     INFO     initialized model with hyperparametrs:\n",
      "2018-10-06 16:11:39,702 __main__     INFO     LR: 0.01\n",
      "2018-10-06 16:11:39,704 __main__     INFO     LR_DECAY_RATE: 0.9\n",
      "2018-10-06 16:11:39,705 __main__     INFO     NEPOCH: 10\n",
      "2018-10-06 16:11:39,705 __main__     INFO     BATCH_SIZE: 32\n",
      "2018-10-06 16:11:39,706 __main__     INFO     NGRAM_SIZE: 2\n",
      "2018-10-06 16:11:39,706 __main__     INFO     VOC_SIZE: 10000\n",
      "2018-10-06 16:11:39,707 __main__     INFO     EMBEDDING_DIM: 100\n",
      "2018-10-06 16:11:39,707 __main__     INFO     NGRAM_MODE: naive\n",
      "2018-10-06 16:11:39,708 __main__     INFO     VAL_SIZE: 5000\n",
      "2018-10-06 16:11:39,709 __main__     INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "2018-10-06 16:11:39,709 __main__     INFO     VAL_FREQ: 4\n",
      "2018-10-06 16:11:39,710 __main__     INFO     REMOVE_STOP_WORDS: True\n",
      "2018-10-06 16:11:39,710 __main__     INFO     REMOVE_PUNC: True\n",
      "2018-10-06 16:11:39,711 __main__     INFO     EARLY_STOP: True\n",
      "2018-10-06 16:11:39,711 __main__     INFO     EARLY_STOP_LOOKBACK: 4\n",
      "2018-10-06 16:11:39,712 __main__     INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "2018-10-06 16:11:39,712 __main__     INFO     allow pickle loads: True, allow pickle saves: True\n",
      "2018-10-06 16:11:40,063 __main__     INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "2018-10-06 16:11:43,827 __main__     INFO     setting each dataset's token indexes\n",
      "2018-10-06 16:11:44,566 __main__     INFO     setting each dataset's token indexes\n",
      "2018-10-06 16:11:48,462 __main__     INFO     Epoch: [1/10], Step: [128/625], Val Acc: 77.96, LR: 0.0100\n",
      "2018-10-06 16:11:50,643 __main__     INFO     Epoch: [1/10], Step: [256/625], Val Acc: 83.6, LR: 0.0100\n",
      "2018-10-06 16:11:54,165 __main__     INFO     Epoch: [1/10], Step: [384/625], Val Acc: 86.76, LR: 0.0100\n",
      "2018-10-06 16:11:56,191 __main__     INFO     Epoch: [1/10], Step: [512/625], Val Acc: 87.12, LR: 0.0100\n",
      "2018-10-06 16:11:59,545 __main__     INFO     Epoch: [2/10], Step: [128/625], Val Acc: 86.72, LR: 0.0090\n",
      "2018-10-06 16:12:01,920 __main__     INFO     Epoch: [2/10], Step: [256/625], Val Acc: 87.32, LR: 0.0090\n",
      "2018-10-06 16:12:05,538 __main__     INFO     Epoch: [2/10], Step: [384/625], Val Acc: 84.8, LR: 0.0090\n",
      "2018-10-06 16:12:08,622 __main__     INFO     Epoch: [2/10], Step: [512/625], Val Acc: 86.34, LR: 0.0090\n",
      "2018-10-06 16:12:12,585 __main__     INFO     Epoch: [3/10], Step: [128/625], Val Acc: 87.34, LR: 0.0081\n",
      "2018-10-06 16:12:14,532 __main__     INFO     Epoch: [3/10], Step: [256/625], Val Acc: 86.58, LR: 0.0081\n",
      "2018-10-06 16:12:17,624 __main__     INFO     Epoch: [3/10], Step: [384/625], Val Acc: 86.9, LR: 0.0081\n",
      "2018-10-06 16:12:20,162 __main__     INFO     Epoch: [3/10], Step: [512/625], Val Acc: 86.4, LR: 0.0081\n",
      "2018-10-06 16:12:22,025 __main__     INFO     generating new pandas dataframe to store results\n",
      "time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "# training all of these through 1 epoch and seeing results\n",
    "reload(mm_mod)\n",
    "mm = mm_mod.ModelManager()\n",
    "mm.load_data()\n",
    "mm.data_to_pipe()\n",
    "param_overrides = {'EARLY_STOP': False}\n",
    "mm.hparams.update(param_overrides)\n",
    "mm.train(epoch_override=3, reload_data=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to find a good LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list_exp_neg = np.arange(1,6)\n",
    "lr_list_neg = 1 / np.power(10, lr_list_exp_neg)\n",
    "lr_list_exp_pos = np.arange(0,3)\n",
    "lr_list_pos = np.power(10, lr_list_exp_pos)\n",
    "\n",
    "lr_list = np.append(lr_list_neg, lr_list_pos)\n",
    "lr_list.sort()\n",
    "print(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all of these through 1 epoch and seeing results\n",
    "mm = mm_mod.ModelManager()\n",
    "mm.load_data()\n",
    "mm.data_to_pipe()\n",
    "\n",
    "mm.res_df = None  # reset the results dataframe\n",
    "for cur_lr in lr_list:\n",
    "    # overriding some hyperparameters\n",
    "    print(\"training for initial lr = %s\" % cur_lr)\n",
    "    param_overrides = {'LR': cur_lr,\n",
    "                       'EARLY_STOP': False}\n",
    "    mm.hparams.update(param_overrides)\n",
    "    mm.train(epoch_override=1, reload_data=False)  \n",
    "display(mm.res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log10(mm.res_df['LR']), mm.res_df['final_val_acc'])\n",
    "plt.title('Validation Error after 1 epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for each ngram param, find the right vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.WARNING)\n",
    "voc_sizes = np.arange(1, 9) * 10000\n",
    "n_list = (1, 2, 3, 4)\n",
    "mode_list = ('naive', 'spacy')\n",
    "\n",
    "for n in n_list:\n",
    "    for mode in mode_list:\n",
    "        for voc_size in voc_sizes:\n",
    "            start_time = time.time()\n",
    "            print(\"training models for: n=%s, mode=%s, voc_size=%s\" % (n, mode, voc_size))\n",
    "            param_overrides = {'NGRAM_MODE': mode,\n",
    "                               'NGRAM_SIZE': n,\n",
    "                               'VOC_SIZE': voc_size}\n",
    "            mm = mm_mod.ModelManager(hparams=param_overrides, res_name='vocab_explore.p')\n",
    "            mm.train()\n",
    "            print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                                  time.time() - start_time))\n",
    "    \n",
    "            mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra vocabulary - for spacy ngram =4, the upper tail hasn't been fully explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(9, 15) * 10000\n",
    "for voc_size in voc_sizes:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: n=4, mode=spacy, voc_size=%s\" % (voc_size))\n",
    "    param_overrides = {'NGRAM_MODE': 'spacy',\n",
    "                       'NGRAM_SIZE': 4,\n",
    "                       'VOC_SIZE': voc_size}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='voc_additional.p')\n",
    "    mm.train()\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(3, 11) * 100000\n",
    "voc_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we tried even larger vocabsizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(3, 11) * 100000\n",
    "for voc_size in voc_sizes:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: n=4, mode=spacy, voc_size=%s\" % (voc_size))\n",
    "    param_overrides = {'NGRAM_MODE': 'spacy',\n",
    "                       'NGRAM_SIZE': 4,\n",
    "                       'VOC_SIZE': voc_size}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='voc_additional.p')\n",
    "    mm.train()\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dims = np.arange(2, 15) * 50\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.WARNING)\n",
    "voc_sizes = np.arange(2, 13) * 10000\n",
    "emb_dims = np.arange(1, 15) * 50\n",
    "\n",
    "for emb_dim in emb_dims:\n",
    "    for voc_size in voc_sizes:\n",
    "        start_time = time.time()\n",
    "        print(\"training models for: emb_dim=%s, voc_size=%s\" % (emb_dim, voc_size))\n",
    "        param_overrides = {'VOC_SIZE': voc_size,\n",
    "                           'NGRAM_MODE':'spacy',\n",
    "                           'EMBEDDING_DIM':emb_dim}\n",
    "        mm = mm_mod.ModelManager(hparams=param_overrides, res_name='embdim.p')\n",
    "        mm.train()\n",
    "        print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                              time.time() - start_time))\n",
    "\n",
    "        mm.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mm.res_df['final_val_acc'].sort_values().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mm.res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_list = [torch.optim.RMSprop, torch.optim.Adagrad, torch.optim.Adam]\n",
    "\n",
    "for opt in opt_list:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: optimizer = %s\" % (str(opt)))\n",
    "    param_overrides = {'OPTIMIZER': opt}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='optim.p')\n",
    "    mm.train()\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just trying a big model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-06 16:57:01,165 __main__     INFO     initialized model with hyperparametrs:\n",
      "2018-10-06 16:57:01,167 __main__     INFO     LR: 0.01\n",
      "2018-10-06 16:57:01,169 __main__     INFO     LR_DECAY_RATE: 0.75\n",
      "2018-10-06 16:57:01,171 __main__     INFO     NEPOCH: 50\n",
      "2018-10-06 16:57:01,172 __main__     INFO     BATCH_SIZE: 32\n",
      "2018-10-06 16:57:01,173 __main__     INFO     NGRAM_SIZE: 4\n",
      "2018-10-06 16:57:01,174 __main__     INFO     VOC_SIZE: 1000000\n",
      "2018-10-06 16:57:01,175 __main__     INFO     EMBEDDING_DIM: 1000\n",
      "2018-10-06 16:57:01,175 __main__     INFO     NGRAM_MODE: spacy\n",
      "2018-10-06 16:57:01,176 __main__     INFO     VAL_SIZE: 5000\n",
      "2018-10-06 16:57:01,177 __main__     INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "2018-10-06 16:57:01,177 __main__     INFO     VAL_FREQ: 4\n",
      "2018-10-06 16:57:01,178 __main__     INFO     REMOVE_STOP_WORDS: True\n",
      "2018-10-06 16:57:01,178 __main__     INFO     REMOVE_PUNC: True\n",
      "2018-10-06 16:57:01,179 __main__     INFO     EARLY_STOP: True\n",
      "2018-10-06 16:57:01,180 __main__     INFO     EARLY_STOP_LOOKBACK: 32\n",
      "2018-10-06 16:57:01,180 __main__     INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "2018-10-06 16:57:01,181 __main__     INFO     allow pickle loads: True, allow pickle saves: True\n",
      "2018-10-06 16:57:01,870 __main__     INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "2018-10-06 16:57:07,967 __main__     INFO     constructing ngram_indexer ...\n",
      "2018-10-06 16:57:07,972 __main__     INFO     indexer length 20000\n",
      "2018-10-06 16:57:14,498 __main__     INFO     final vocal size: 1000002\n",
      "2018-10-06 16:57:14,499 __main__     INFO     saving pickled indexer to folder ./data/pickles/ ...\n",
      "2018-10-06 16:57:14,871 __main__     INFO     setting each dataset's token indexes\n",
      "2018-10-06 16:57:17,285 __main__     INFO     setting each dataset's token indexes\n",
      "2018-10-06 17:04:58,677 __main__     INFO     Epoch: [1/50], Step: [128/625], Val Acc: 80.7, LR: 0.0100\n",
      "2018-10-06 17:12:07,672 __main__     INFO     Epoch: [1/50], Step: [256/625], Val Acc: 87.1, LR: 0.0100\n",
      "2018-10-06 17:19:02,524 __main__     INFO     Epoch: [1/50], Step: [384/625], Val Acc: 80.16, LR: 0.0100\n",
      "2018-10-06 17:25:47,226 __main__     INFO     Epoch: [1/50], Step: [512/625], Val Acc: 87.8, LR: 0.0100\n",
      "2018-10-06 17:41:45,102 __main__     INFO     Epoch: [2/50], Step: [128/625], Val Acc: 89.7, LR: 0.0075\n",
      "2018-10-06 17:55:48,594 __main__     INFO     Epoch: [2/50], Step: [256/625], Val Acc: 88.32, LR: 0.0075\n",
      "2018-10-06 18:10:14,480 __main__     INFO     Epoch: [2/50], Step: [384/625], Val Acc: 89.66, LR: 0.0075\n",
      "2018-10-06 18:22:16,169 __main__     INFO     Epoch: [2/50], Step: [512/625], Val Acc: 87.96, LR: 0.0075\n"
     ]
    }
   ],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.INFO)\n",
    "start_time = time.time()\n",
    "param_overrides = {'NEPOCH': 50,\n",
    "                   'VOC_SIZE': 1000000,\n",
    "                   'NGRAM_SIZE': 4,\n",
    "                   'NGRAM_MODE':'spacy',\n",
    "                   'EMBEDDING_DIM':1000,\n",
    "                   'LR_DECAY_RATE': 0.75,\n",
    "                   'EARLY_STOP_LOOKBACK': 32}\n",
    "mm = mm_mod.ModelManager(hparams=param_overrides, res_name='experiment.p')\n",
    "mm.train()\n",
    "print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                      time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
