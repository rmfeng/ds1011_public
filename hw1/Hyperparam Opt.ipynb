{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ModelManager as mm_mod\n",
    "import config_defaults as cd\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model and Data pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.91 ms\n"
     ]
    }
   ],
   "source": [
    "reload(mm_mod)\n",
    "reload(cd)\n",
    "logger = logging.getLogger('__main__')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting the ngrams for n = 1, 2, 3, 4 with both naive and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n_list = (1, 2, 3, 4)\n",
    "mode_list = ('naive', 'spacy')\n",
    "\n",
    "for n in n_list:\n",
    "    for mode in mode_list:\n",
    "        print(\"extracting n-grams for: n=%s, mode=%s\" % (n, mode))\n",
    "        param_overrides = {'NGRAM_MODE': mode,\n",
    "                           'NGRAM_SIZE': n}\n",
    "        mm = mm_mod.ModelManager(hparams=param_overrides)\n",
    "        mm.load_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the annealing of LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all of these through 1 epoch and seeing results\n",
    "reload(mm_mod)\n",
    "mm = mm_mod.ModelManager()\n",
    "mm.load_data()\n",
    "mm.data_to_pipe()\n",
    "param_overrides = {'EARLY_STOP': False}\n",
    "mm.hparams.update(param_overrides)\n",
    "mm.train(epoch_override=3, reload_data=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to find a good LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list_exp_neg = np.arange(1,6)\n",
    "lr_list_neg = 1 / np.power(10, lr_list_exp_neg)\n",
    "lr_list_exp_pos = np.arange(0,3)\n",
    "lr_list_pos = np.power(10, lr_list_exp_pos)\n",
    "\n",
    "lr_list = np.append(lr_list_neg, lr_list_pos)\n",
    "lr_list.sort()\n",
    "print(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all of these through 1 epoch and seeing results\n",
    "mm = mm_mod.ModelManager()\n",
    "mm.load_data()\n",
    "mm.data_to_pipe()\n",
    "\n",
    "mm.res_df = None  # reset the results dataframe\n",
    "for cur_lr in lr_list:\n",
    "    # overriding some hyperparameters\n",
    "    print(\"training for initial lr = %s\" % cur_lr)\n",
    "    param_overrides = {'LR': cur_lr,\n",
    "                       'EARLY_STOP': False}\n",
    "    mm.hparams.update(param_overrides)\n",
    "    mm.train(epoch_override=1, reload_data=False)  \n",
    "display(mm.res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log10(mm.res_df['LR']), mm.res_df['final_val_acc'])\n",
    "plt.title('Validation Error after 1 epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for each ngram param, find the right vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.WARNING)\n",
    "voc_sizes = np.arange(1, 9) * 10000\n",
    "n_list = (1, 2, 3, 4)\n",
    "mode_list = ('naive', 'spacy')\n",
    "\n",
    "for n in n_list:\n",
    "    for mode in mode_list:\n",
    "        for voc_size in voc_sizes:\n",
    "            start_time = time.time()\n",
    "            print(\"training models for: n=%s, mode=%s, voc_size=%s\" % (n, mode, voc_size))\n",
    "            param_overrides = {'NGRAM_MODE': mode,\n",
    "                               'NGRAM_SIZE': n,\n",
    "                               'VOC_SIZE': voc_size}\n",
    "            mm = mm_mod.ModelManager(hparams=param_overrides, res_name='vocab_explore.p')\n",
    "            mm.train()\n",
    "            print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                                  time.time() - start_time))\n",
    "    \n",
    "            mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra vocabulary - for spacy ngram =4, the upper tail hasn't been fully explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(9, 15) * 10000\n",
    "for voc_size in voc_sizes:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: n=4, mode=spacy, voc_size=%s\" % (voc_size))\n",
    "    param_overrides = {'NGRAM_MODE': 'spacy',\n",
    "                       'NGRAM_SIZE': 4,\n",
    "                       'VOC_SIZE': voc_size}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='voc_additional.p')\n",
    "    mm.train()\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(3, 11) * 100000\n",
    "voc_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we tried even larger vocabsizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sizes = np.arange(3, 11) * 100000\n",
    "for voc_size in voc_sizes:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: n=4, mode=spacy, voc_size=%s\" % (voc_size))\n",
    "    param_overrides = {'NGRAM_MODE': 'spacy',\n",
    "                       'NGRAM_SIZE': 4,\n",
    "                       'VOC_SIZE': voc_size}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='voc_additional.p')\n",
    "    mm.train()\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dims = np.arange(2, 15) * 50\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.WARNING)\n",
    "voc_sizes = np.arange(2, 13) * 10000\n",
    "emb_dims = np.arange(1, 15) * 50\n",
    "\n",
    "for emb_dim in emb_dims:\n",
    "    for voc_size in voc_sizes:\n",
    "        start_time = time.time()\n",
    "        print(\"training models for: emb_dim=%s, voc_size=%s\" % (emb_dim, voc_size))\n",
    "        param_overrides = {'VOC_SIZE': voc_size,\n",
    "                           'NGRAM_MODE':'spacy',\n",
    "                           'EMBEDDING_DIM':emb_dim}\n",
    "        mm = mm_mod.ModelManager(hparams=param_overrides, res_name='embdim.p')\n",
    "        mm.train()\n",
    "        print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.validation_acc_history[-1], \n",
    "                                                              time.time() - start_time))\n",
    "\n",
    "        mm.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mm.res_df['final_val_acc'].sort_values().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mm.res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 413 Âµs\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training models for: optimizer = <class 'torch.optim.rmsprop.RMSprop'>\n",
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.01\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 10\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 100000\n",
      "INFO     EMBEDDING_DIM: 50\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: False\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 8\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     Starting Training on device: cuda:0\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_False.p\n",
      "./data/pickles/test_spacy_4_True_False.p\n",
      "./data/pickles/idx_spacy_4_True_False_5000_100000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     found pickle files for indexer in ./data/pickles/, loading them ... \n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     Ep: [1/10], Sp: [128/625], VAcc: 84.36, VLoss: 83.3, TAcc: 86.03, TLoss: 322.1, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [256/625], VAcc: 86.86, VLoss: 74.2, TAcc: 90.43, TLoss: 277.6, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [384/625], VAcc: 84.76, VLoss: 74.0, TAcc: 89.445, TLoss: 271.9, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [512/625], VAcc: 88.5, VLoss: 69.4, TAcc: 94.6, TLoss: 246.8, LR: 0.0100\n",
      "INFO     Ep: [2/10], Sp: [128/625], VAcc: 89.0, VLoss: 67.1, TAcc: 96.25, TLoss: 229.7, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [256/625], VAcc: 89.54, VLoss: 66.4, TAcc: 97.125, TLoss: 223.5, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [384/625], VAcc: 90.34, VLoss: 65.3, TAcc: 98.015, TLoss: 216.2, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [512/625], VAcc: 89.86, VLoss: 65.4, TAcc: 98.53, TLoss: 212.9, LR: 0.0095\n",
      "INFO     found historical file, loading the dataframe at ./results/optim.p\n",
      "Final Validation Acc = 89.86 (train time: 70.9s)\n",
      "\n",
      "INFO     results saved to ./results/optim.p\n",
      "training models for: optimizer = <class 'torch.optim.adagrad.Adagrad'>\n",
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.01\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 10\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 100000\n",
      "INFO     EMBEDDING_DIM: 50\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.adagrad.Adagrad'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: False\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 8\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     Starting Training on device: cuda:0\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_False.p\n",
      "./data/pickles/test_spacy_4_True_False.p\n",
      "./data/pickles/idx_spacy_4_True_False_5000_100000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     found pickle files for indexer in ./data/pickles/, loading them ... \n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     Ep: [1/10], Sp: [128/625], VAcc: 50.08, VLoss: 108.3, TAcc: 50.04, TLoss: 430.9, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [256/625], VAcc: 57.08, VLoss: 107.7, TAcc: 59.005, TLoss: 428.2, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [384/625], VAcc: 75.28, VLoss: 107.1, TAcc: 78.485, TLoss: 425.6, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [512/625], VAcc: 55.9, VLoss: 106.6, TAcc: 57.81, TLoss: 423.2, LR: 0.0100\n",
      "INFO     Ep: [2/10], Sp: [128/625], VAcc: 77.74, VLoss: 105.3, TAcc: 79.82, TLoss: 417.3, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [256/625], VAcc: 70.16, VLoss: 104.7, TAcc: 72.06, TLoss: 414.7, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [384/625], VAcc: 78.6, VLoss: 103.9, TAcc: 81.655, TLoss: 411.0, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [512/625], VAcc: 79.18, VLoss: 103.2, TAcc: 82.18, TLoss: 408.0, LR: 0.0095\n",
      "INFO     found historical file, loading the dataframe at ./results/optim.p\n",
      "Final Validation Acc = 79.18 (train time: 74.7s)\n",
      "\n",
      "INFO     results saved to ./results/optim.p\n",
      "training models for: optimizer = <class 'torch.optim.adam.Adam'>\n",
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.01\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 10\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 100000\n",
      "INFO     EMBEDDING_DIM: 50\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: False\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 8\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     Starting Training on device: cuda:0\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_False.p\n",
      "./data/pickles/test_spacy_4_True_False.p\n",
      "./data/pickles/idx_spacy_4_True_False_5000_100000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     found pickle files for indexer in ./data/pickles/, loading them ... \n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     Ep: [1/10], Sp: [128/625], VAcc: 79.6, VLoss: 94.1, TAcc: 81.825, TLoss: 368.4, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [256/625], VAcc: 86.44, VLoss: 77.0, TAcc: 90.38, TLoss: 289.2, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [384/625], VAcc: 88.0, VLoss: 72.1, TAcc: 93.155, TLoss: 262.6, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [512/625], VAcc: 89.32, VLoss: 69.7, TAcc: 95.22, TLoss: 246.6, LR: 0.0100\n",
      "INFO     Ep: [2/10], Sp: [128/625], VAcc: 90.36, VLoss: 66.7, TAcc: 97.495, TLoss: 226.2, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [256/625], VAcc: 90.48, VLoss: 66.2, TAcc: 97.865, TLoss: 221.8, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [384/625], VAcc: 90.88, VLoss: 65.5, TAcc: 98.645, TLoss: 215.5, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [512/625], VAcc: 90.78, VLoss: 65.0, TAcc: 99.165, TLoss: 210.3, LR: 0.0095\n",
      "INFO     found historical file, loading the dataframe at ./results/optim.p\n",
      "Final Validation Acc = 90.78 (train time: 74.5s)\n",
      "\n",
      "INFO     results saved to ./results/optim.p\n",
      "training models for: optimizer = <class 'torch.optim.sgd.SGD'>\n",
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.01\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 10\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 100000\n",
      "INFO     EMBEDDING_DIM: 50\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.sgd.SGD'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: False\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 8\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     Starting Training on device: cuda:0\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_False.p\n",
      "./data/pickles/test_spacy_4_True_False.p\n",
      "./data/pickles/idx_spacy_4_True_False_5000_100000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     found pickle files for indexer in ./data/pickles/, loading them ... \n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     Ep: [1/10], Sp: [128/625], VAcc: 49.92, VLoss: 108.9, TAcc: 49.94, TLoss: 433.4, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [256/625], VAcc: 48.82, VLoss: 108.8, TAcc: 49.23, TLoss: 433.3, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [384/625], VAcc: 50.0, VLoss: 108.9, TAcc: 49.99, TLoss: 433.4, LR: 0.0100\n",
      "INFO     Ep: [1/10], Sp: [512/625], VAcc: 49.92, VLoss: 108.8, TAcc: 49.95, TLoss: 433.2, LR: 0.0100\n",
      "INFO     Ep: [2/10], Sp: [128/625], VAcc: 50.4, VLoss: 108.8, TAcc: 50.28, TLoss: 433.2, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [256/625], VAcc: 50.06, VLoss: 108.8, TAcc: 50.02, TLoss: 433.2, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [384/625], VAcc: 50.98, VLoss: 108.8, TAcc: 50.21, TLoss: 433.2, LR: 0.0095\n",
      "INFO     Ep: [2/10], Sp: [512/625], VAcc: 51.34, VLoss: 108.8, TAcc: 50.27, TLoss: 433.2, LR: 0.0095\n",
      "INFO     found historical file, loading the dataframe at ./results/optim.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Acc = 51.34 (train time: 71.9s)\n",
      "\n",
      "INFO     results saved to ./results/optim.p\n",
      "time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "opt_list = [torch.optim.RMSprop, torch.optim.Adagrad, torch.optim.Adam, torch.optim.SGD]\n",
    "first_loop = True\n",
    "\n",
    "for opt in opt_list:\n",
    "    start_time = time.time()\n",
    "    print(\"training models for: optimizer = %s\" % (str(opt)))\n",
    "    param_overrides = {'OPTIMIZER': opt}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='optim.p')\n",
    "    mm.train(epoch_override=2)\n",
    "    print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.val_acc_hist[-1], \n",
    "                                                          time.time() - start_time))\n",
    "\n",
    "    mm.save_results()\n",
    "    first_loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different LR decay rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_decays = 0.5 + np.arange(2, 11) * 0.05\n",
    "\n",
    "for lr_decay in lr_decays:\n",
    "    param_overrides = {'LR_DECAY_RATE': lr_decay,\n",
    "                       'LR': 0.001,\n",
    "                       'NEPOCH': 30}\n",
    "    mm = mm_mod.ModelManager(hparams=param_overrides, res_name='lr_decay_small_lr.p')\n",
    "    mm.train()\n",
    "    mm.save_results()\n",
    "\n",
    "print(\"Final Validation Acc = %s\" % (mm.validation_acc_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final massive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.001\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 50\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 1000000\n",
      "INFO     EMBEDDING_DIM: 100\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: False\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 32\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     Starting Training on device: cuda:0\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_False.p\n",
      "./data/pickles/test_spacy_4_True_False.p\n",
      "./data/pickles/idx_spacy_4_True_False_5000_1000000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     constructing ngram_indexer ...\n",
      "INFO     indexer length 20000\n",
      "INFO     final vocal size: 1000002\n",
      "INFO     saving pickled indexer to folder ./data/pickles/ ...\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     Ep: [1/50], Sp: [128/625], VAcc: 63.02, VLoss: 108.2, TAcc: 63.995, TLoss: 430.3, LR: 0.0010\n",
      "INFO     Ep: [1/50], Sp: [256/625], VAcc: 65.4, VLoss: 107.1, TAcc: 68.74, TLoss: 425.1, LR: 0.0010\n",
      "INFO     Ep: [1/50], Sp: [384/625], VAcc: 78.06, VLoss: 105.0, TAcc: 82.59, TLoss: 414.5, LR: 0.0010\n",
      "INFO     Ep: [1/50], Sp: [512/625], VAcc: 78.76, VLoss: 101.6, TAcc: 83.58, TLoss: 397.8, LR: 0.0010\n",
      "INFO     Ep: [2/50], Sp: [128/625], VAcc: 80.98, VLoss: 93.7, TAcc: 87.22, TLoss: 357.2, LR: 0.0009\n",
      "INFO     Ep: [2/50], Sp: [256/625], VAcc: 84.12, VLoss: 89.7, TAcc: 90.135, TLoss: 336.5, LR: 0.0009\n",
      "INFO     Ep: [2/50], Sp: [384/625], VAcc: 83.92, VLoss: 86.6, TAcc: 90.83, TLoss: 319.1, LR: 0.0009\n",
      "INFO     Ep: [2/50], Sp: [512/625], VAcc: 85.62, VLoss: 83.8, TAcc: 93.045, TLoss: 304.1, LR: 0.0009\n",
      "INFO     Ep: [3/50], Sp: [128/625], VAcc: 86.94, VLoss: 79.9, TAcc: 94.83, TLoss: 281.7, LR: 0.0009\n",
      "INFO     Ep: [3/50], Sp: [256/625], VAcc: 87.38, VLoss: 78.3, TAcc: 95.43, TLoss: 272.7, LR: 0.0009\n",
      "INFO     Ep: [3/50], Sp: [384/625], VAcc: 87.36, VLoss: 77.0, TAcc: 95.97, TLoss: 264.8, LR: 0.0009\n",
      "INFO     Ep: [3/50], Sp: [512/625], VAcc: 87.68, VLoss: 75.9, TAcc: 96.52, TLoss: 257.5, LR: 0.0009\n",
      "INFO     Ep: [4/50], Sp: [128/625], VAcc: 88.28, VLoss: 74.1, TAcc: 97.39, TLoss: 246.8, LR: 0.0009\n",
      "INFO     Ep: [4/50], Sp: [256/625], VAcc: 88.42, VLoss: 73.5, TAcc: 97.805, TLoss: 242.2, LR: 0.0009\n",
      "INFO     Ep: [4/50], Sp: [384/625], VAcc: 88.74, VLoss: 72.8, TAcc: 98.005, TLoss: 237.8, LR: 0.0009\n",
      "INFO     Ep: [4/50], Sp: [512/625], VAcc: 88.52, VLoss: 72.1, TAcc: 98.38, TLoss: 233.8, LR: 0.0009\n",
      "INFO     Ep: [5/50], Sp: [128/625], VAcc: 89.08, VLoss: 71.1, TAcc: 98.895, TLoss: 227.3, LR: 0.0008\n",
      "INFO     Ep: [5/50], Sp: [256/625], VAcc: 89.16, VLoss: 70.7, TAcc: 99.03, TLoss: 224.6, LR: 0.0008\n",
      "INFO     Ep: [5/50], Sp: [384/625], VAcc: 89.34, VLoss: 70.3, TAcc: 99.13, TLoss: 222.1, LR: 0.0008\n",
      "INFO     Ep: [5/50], Sp: [512/625], VAcc: 89.42, VLoss: 69.9, TAcc: 99.335, TLoss: 219.6, LR: 0.0008\n",
      "INFO     Ep: [6/50], Sp: [128/625], VAcc: 89.54, VLoss: 69.3, TAcc: 99.51, TLoss: 215.7, LR: 0.0008\n",
      "INFO     Ep: [6/50], Sp: [256/625], VAcc: 89.72, VLoss: 69.0, TAcc: 99.595, TLoss: 214.0, LR: 0.0008\n",
      "INFO     Ep: [6/50], Sp: [384/625], VAcc: 89.66, VLoss: 68.8, TAcc: 99.645, TLoss: 212.4, LR: 0.0008\n",
      "INFO     Ep: [6/50], Sp: [512/625], VAcc: 89.86, VLoss: 68.5, TAcc: 99.715, TLoss: 210.9, LR: 0.0008\n",
      "INFO     Ep: [7/50], Sp: [128/625], VAcc: 89.96, VLoss: 68.1, TAcc: 99.8, TLoss: 208.4, LR: 0.0007\n",
      "INFO     Ep: [7/50], Sp: [256/625], VAcc: 90.04, VLoss: 67.8, TAcc: 99.83, TLoss: 207.4, LR: 0.0007\n",
      "INFO     Ep: [7/50], Sp: [384/625], VAcc: 90.08, VLoss: 67.6, TAcc: 99.86, TLoss: 206.4, LR: 0.0007\n",
      "INFO     Ep: [7/50], Sp: [512/625], VAcc: 90.18, VLoss: 67.5, TAcc: 99.875, TLoss: 205.6, LR: 0.0007\n",
      "INFO     Ep: [8/50], Sp: [128/625], VAcc: 90.18, VLoss: 67.2, TAcc: 99.93, TLoss: 204.1, LR: 0.0007\n",
      "INFO     Ep: [8/50], Sp: [256/625], VAcc: 90.2, VLoss: 67.0, TAcc: 99.945, TLoss: 203.4, LR: 0.0007\n",
      "INFO     Ep: [8/50], Sp: [384/625], VAcc: 90.26, VLoss: 66.9, TAcc: 99.95, TLoss: 202.8, LR: 0.0007\n",
      "INFO     Ep: [8/50], Sp: [512/625], VAcc: 90.28, VLoss: 66.7, TAcc: 99.97, TLoss: 202.2, LR: 0.0007\n",
      "INFO     Ep: [9/50], Sp: [128/625], VAcc: 90.32, VLoss: 66.5, TAcc: 99.975, TLoss: 201.3, LR: 0.0007\n",
      "INFO     Ep: [9/50], Sp: [256/625], VAcc: 90.32, VLoss: 66.4, TAcc: 99.975, TLoss: 200.9, LR: 0.0007\n",
      "INFO     Ep: [9/50], Sp: [384/625], VAcc: 90.34, VLoss: 66.3, TAcc: 99.975, TLoss: 200.5, LR: 0.0007\n",
      "INFO     Ep: [9/50], Sp: [512/625], VAcc: 90.34, VLoss: 66.2, TAcc: 99.98, TLoss: 200.1, LR: 0.0007\n",
      "INFO     Ep: [10/50], Sp: [128/625], VAcc: 90.42, VLoss: 66.0, TAcc: 99.98, TLoss: 199.5, LR: 0.0006\n",
      "INFO     Ep: [10/50], Sp: [256/625], VAcc: 90.34, VLoss: 65.9, TAcc: 99.985, TLoss: 199.3, LR: 0.0006\n",
      "INFO     Ep: [10/50], Sp: [384/625], VAcc: 90.42, VLoss: 65.8, TAcc: 99.985, TLoss: 199.0, LR: 0.0006\n",
      "INFO     Ep: [10/50], Sp: [512/625], VAcc: 90.46, VLoss: 65.7, TAcc: 99.99, TLoss: 198.8, LR: 0.0006\n",
      "INFO     Ep: [11/50], Sp: [128/625], VAcc: 90.52, VLoss: 65.6, TAcc: 99.99, TLoss: 198.4, LR: 0.0006\n",
      "INFO     Ep: [11/50], Sp: [256/625], VAcc: 90.6, VLoss: 65.5, TAcc: 99.99, TLoss: 198.2, LR: 0.0006\n",
      "INFO     Ep: [11/50], Sp: [384/625], VAcc: 90.5, VLoss: 65.4, TAcc: 99.99, TLoss: 198.1, LR: 0.0006\n",
      "INFO     Ep: [11/50], Sp: [512/625], VAcc: 90.56, VLoss: 65.4, TAcc: 99.99, TLoss: 197.9, LR: 0.0006\n",
      "INFO     Ep: [12/50], Sp: [128/625], VAcc: 90.56, VLoss: 65.3, TAcc: 99.99, TLoss: 197.7, LR: 0.0006\n",
      "INFO     Ep: [12/50], Sp: [256/625], VAcc: 90.58, VLoss: 65.2, TAcc: 99.99, TLoss: 197.5, LR: 0.0006\n",
      "INFO     Ep: [12/50], Sp: [384/625], VAcc: 90.66, VLoss: 65.1, TAcc: 99.99, TLoss: 197.4, LR: 0.0006\n",
      "INFO     Ep: [12/50], Sp: [512/625], VAcc: 90.62, VLoss: 65.1, TAcc: 99.99, TLoss: 197.3, LR: 0.0006\n",
      "INFO     Ep: [13/50], Sp: [128/625], VAcc: 90.62, VLoss: 65.0, TAcc: 99.995, TLoss: 197.1, LR: 0.0005\n",
      "INFO     Ep: [13/50], Sp: [256/625], VAcc: 90.6, VLoss: 64.9, TAcc: 99.995, TLoss: 197.1, LR: 0.0005\n",
      "INFO     Ep: [13/50], Sp: [384/625], VAcc: 90.62, VLoss: 64.9, TAcc: 99.995, TLoss: 197.0, LR: 0.0005\n",
      "INFO     Ep: [13/50], Sp: [512/625], VAcc: 90.66, VLoss: 64.8, TAcc: 99.995, TLoss: 196.9, LR: 0.0005\n",
      "INFO     Ep: [14/50], Sp: [128/625], VAcc: 90.7, VLoss: 64.7, TAcc: 99.995, TLoss: 196.8, LR: 0.0005\n",
      "INFO     Ep: [14/50], Sp: [256/625], VAcc: 90.6, VLoss: 64.7, TAcc: 100.0, TLoss: 196.7, LR: 0.0005\n",
      "INFO     Ep: [14/50], Sp: [384/625], VAcc: 90.72, VLoss: 64.6, TAcc: 100.0, TLoss: 196.6, LR: 0.0005\n",
      "INFO     Ep: [14/50], Sp: [512/625], VAcc: 90.7, VLoss: 64.6, TAcc: 100.0, TLoss: 196.6, LR: 0.0005\n",
      "INFO     Ep: [15/50], Sp: [128/625], VAcc: 90.7, VLoss: 64.5, TAcc: 100.0, TLoss: 196.5, LR: 0.0005\n",
      "INFO     Ep: [15/50], Sp: [256/625], VAcc: 90.74, VLoss: 64.5, TAcc: 100.0, TLoss: 196.5, LR: 0.0005\n",
      "INFO     Ep: [15/50], Sp: [384/625], VAcc: 90.7, VLoss: 64.4, TAcc: 100.0, TLoss: 196.4, LR: 0.0005\n",
      "INFO     Ep: [15/50], Sp: [512/625], VAcc: 90.68, VLoss: 64.4, TAcc: 100.0, TLoss: 196.4, LR: 0.0005\n",
      "INFO     Ep: [16/50], Sp: [128/625], VAcc: 90.76, VLoss: 64.3, TAcc: 100.0, TLoss: 196.3, LR: 0.0005\n",
      "INFO     Ep: [16/50], Sp: [256/625], VAcc: 90.74, VLoss: 64.3, TAcc: 100.0, TLoss: 196.3, LR: 0.0005\n",
      "INFO     Ep: [16/50], Sp: [384/625], VAcc: 90.68, VLoss: 64.3, TAcc: 100.0, TLoss: 196.3, LR: 0.0005\n",
      "INFO     Ep: [16/50], Sp: [512/625], VAcc: 90.68, VLoss: 64.3, TAcc: 100.0, TLoss: 196.2, LR: 0.0005\n",
      "INFO     Ep: [17/50], Sp: [128/625], VAcc: 90.74, VLoss: 64.2, TAcc: 100.0, TLoss: 196.2, LR: 0.0004\n",
      "INFO     Ep: [17/50], Sp: [256/625], VAcc: 90.68, VLoss: 64.2, TAcc: 100.0, TLoss: 196.2, LR: 0.0004\n",
      "INFO     Ep: [17/50], Sp: [384/625], VAcc: 90.7, VLoss: 64.1, TAcc: 100.0, TLoss: 196.1, LR: 0.0004\n",
      "INFO     Ep: [17/50], Sp: [512/625], VAcc: 90.72, VLoss: 64.1, TAcc: 100.0, TLoss: 196.1, LR: 0.0004\n",
      "INFO     Ep: [18/50], Sp: [128/625], VAcc: 90.78, VLoss: 64.0, TAcc: 100.0, TLoss: 196.1, LR: 0.0004\n",
      "INFO     Ep: [18/50], Sp: [256/625], VAcc: 90.72, VLoss: 64.0, TAcc: 100.0, TLoss: 196.1, LR: 0.0004\n",
      "INFO     Ep: [18/50], Sp: [384/625], VAcc: 90.72, VLoss: 64.0, TAcc: 100.0, TLoss: 196.1, LR: 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     Ep: [18/50], Sp: [512/625], VAcc: 90.84, VLoss: 64.0, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [19/50], Sp: [128/625], VAcc: 90.78, VLoss: 63.9, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [19/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.9, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [19/50], Sp: [384/625], VAcc: 90.78, VLoss: 63.9, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [19/50], Sp: [512/625], VAcc: 90.8, VLoss: 63.8, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [20/50], Sp: [128/625], VAcc: 90.82, VLoss: 63.8, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [20/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.8, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [20/50], Sp: [384/625], VAcc: 90.82, VLoss: 63.8, TAcc: 100.0, TLoss: 196.0, LR: 0.0004\n",
      "INFO     Ep: [20/50], Sp: [512/625], VAcc: 90.82, VLoss: 63.7, TAcc: 100.0, TLoss: 195.9, LR: 0.0004\n",
      "INFO     Ep: [21/50], Sp: [128/625], VAcc: 90.8, VLoss: 63.7, TAcc: 100.0, TLoss: 195.9, LR: 0.0004\n",
      "INFO     Ep: [21/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.7, TAcc: 100.0, TLoss: 195.9, LR: 0.0004\n",
      "INFO     Ep: [21/50], Sp: [384/625], VAcc: 90.8, VLoss: 63.7, TAcc: 100.0, TLoss: 195.9, LR: 0.0004\n",
      "INFO     Ep: [21/50], Sp: [512/625], VAcc: 90.78, VLoss: 63.7, TAcc: 100.0, TLoss: 195.9, LR: 0.0004\n",
      "INFO     Ep: [22/50], Sp: [128/625], VAcc: 90.8, VLoss: 63.6, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [22/50], Sp: [256/625], VAcc: 90.8, VLoss: 63.6, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [22/50], Sp: [384/625], VAcc: 90.82, VLoss: 63.6, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [22/50], Sp: [512/625], VAcc: 90.88, VLoss: 63.6, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [23/50], Sp: [128/625], VAcc: 90.78, VLoss: 63.6, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [23/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [23/50], Sp: [384/625], VAcc: 90.8, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [23/50], Sp: [512/625], VAcc: 90.84, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [24/50], Sp: [128/625], VAcc: 90.8, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [24/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [24/50], Sp: [384/625], VAcc: 90.74, VLoss: 63.5, TAcc: 100.0, TLoss: 195.9, LR: 0.0003\n",
      "INFO     Ep: [24/50], Sp: [512/625], VAcc: 90.84, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [25/50], Sp: [128/625], VAcc: 90.78, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [25/50], Sp: [256/625], VAcc: 90.82, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [25/50], Sp: [384/625], VAcc: 90.76, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [25/50], Sp: [512/625], VAcc: 90.86, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [26/50], Sp: [128/625], VAcc: 90.72, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n",
      "INFO     Ep: [26/50], Sp: [256/625], VAcc: 90.8, VLoss: 63.4, TAcc: 100.0, TLoss: 195.8, LR: 0.0003\n"
     ]
    }
   ],
   "source": [
    "reload(mm_mod)\n",
    "logger.setLevel(logging.INFO)\n",
    "start_time = time.time()\n",
    "param_overrides = {'NEPOCH': 50,\n",
    "                   'LR': 0.001,\n",
    "                   'LR_DECAY_RATE': 0.95,\n",
    "                   'VOC_SIZE': 1000000,\n",
    "                   'NGRAM_SIZE': 4,\n",
    "                   'NGRAM_MODE':'spacy',\n",
    "                   'EMBEDDING_DIM':100,\n",
    "                   'EARLY_STOP_LOOKBACK': 32}\n",
    "mm = mm_mod.ModelManager(hparams=param_overrides, res_name='experiment.p')\n",
    "mm.train()\n",
    "print(\"Final Validation Acc = %s (train time: %.1fs)\\n\" % (mm.val_acc_hist[-1], \n",
    "                                                      time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mm.model, r'model_state.st')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model from state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.001\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 50\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 4\n",
      "INFO     VOC_SIZE: 1000000\n",
      "INFO     EMBEDDING_DIM: 100\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: True\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 32\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "INFO     looking for the following file paths: ./data/pickles/trainval_spacy_4_True_True.p\n",
      "./data/pickles/test_spacy_4_True_True.p\n",
      "./data/pickles/idx_spacy_4_True_True_5000_1000000.p\n",
      "INFO     found pickle files in ./data/pickles/, loading them instead of rebuilding ... \n",
      "INFO     found pickle files for indexer in ./data/pickles/, loading them ... \n",
      "INFO     setting each dataset's token indexes\n",
      "INFO     setting each dataset's token indexes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(r'model_state.st')\n",
    "\n",
    "param_overrides = {'NEPOCH': 50,\n",
    "                   'LR': 0.001,\n",
    "                   'LR_DECAY_RATE': 0.95,\n",
    "                   'VOC_SIZE': 1000000,\n",
    "                   'NGRAM_SIZE': 4,\n",
    "                   'NGRAM_MODE':'spacy',\n",
    "                   'EMBEDDING_DIM':100,\n",
    "                   'EARLY_STOP_LOOKBACK': 32}\n",
    "mm_test = mm_mod.ModelManager(hparams=param_overrides)\n",
    "mm_test.load_data()\n",
    "mm_test.data_to_pipe()\n",
    "mm_test.model = loaded_model\n",
    "\n",
    "mm_test.test_model(mm_test.loaders['test'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recreating archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_defaults as cd\n",
    "import data_processor as dp\n",
    "import ngrams\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     initialized model with hyperparametrs:\n",
      "INFO     LR: 0.01\n",
      "INFO     LR_DECAY_RATE: 0.95\n",
      "INFO     NEPOCH: 10\n",
      "INFO     BATCH_SIZE: 32\n",
      "INFO     NGRAM_SIZE: 2\n",
      "INFO     VOC_SIZE: 100000\n",
      "INFO     EMBEDDING_DIM: 50\n",
      "INFO     NGRAM_MODE: spacy\n",
      "INFO     VAL_SIZE: 5000\n",
      "INFO     OPTIMIZER: <class 'torch.optim.adam.Adam'>\n",
      "INFO     VAL_FREQ: 4\n",
      "INFO     REMOVE_STOP_WORDS: True\n",
      "INFO     REMOVE_PUNC: True\n",
      "INFO     EARLY_STOP: True\n",
      "INFO     EARLY_STOP_LOOKBACK: 8\n",
      "INFO     EARLY_STOP_MIN_IMPROVE: 0.01\n",
      "INFO     allow pickle loads: True, allow pickle saves: True\n",
      "time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "mm = mm_mod.ModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 549 ms\n"
     ]
    }
   ],
   "source": [
    "test_set = dp.construct_dataset(cd.DIR_TEST, cd.TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ngrams.extract_ngrams(test_set,\n",
    "                                  mm.hparams['NGRAM_SIZE'],\n",
    "                                  remove_stopwords=mm.hparams['REMOVE_STOP_WORDS'],\n",
    "                                  remove_punc=mm.hparams['REMOVE_PUNC'],\n",
    "                                  mode=mm.hparams['NGRAM_MODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparam_to_str(hparams, req_params):\n",
    "    final_str = ''\n",
    "    for key in sorted(hparams):\n",
    "        if key in req_params:\n",
    "            final_str += str(hparams[key]).replace('.', 'p').replace(':', '-') + \"_\"\n",
    "    return final_str[:-1] + '.p'\n",
    "\n",
    "pickle_path_test = cd.DIR_PICKLE + 'test_' + hparam_to_str(mm.hparams, cd.DATA_HPARAMS)\n",
    "pkl.dump(test_data, open(pickle_path_test, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curves "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
