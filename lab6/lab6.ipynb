{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling: KenLM\n",
    "\n",
    "We are going to learn how to use KenLM, a toolkit for language modeling.\n",
    "First of all, you need to install KenLM.\n",
    "- Download and unzip: http://kheafield.com/code/kenlm.tar.gz\n",
    "- You need:\n",
    "    - cmake : https://cmake.org/download/ and unzip.\n",
    "      - Do the following:\n",
    "       ``` cd cmake\n",
    "           ./bootstrap\n",
    "           make\n",
    "           make install\n",
    "       ```\n",
    "    - Need Boost >= 1.42.0 and bjam\n",
    "        - Ubuntu: sudo apt-get install libboost-all-dev\n",
    "        - Mac: brew install boost; brew install bjam\n",
    "- cd into kenlm folder and compiling using the following commands:\n",
    "```bash\n",
    "mkdir -p build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j 4\n",
    "```\n",
    "- Install python KenLM: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "- Check out KenLM website for more info: http://kheafield.com/code/kenlm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model with KenLM\n",
    "Let's train a bigram language model and 4-gram language model.  \n",
    "First, download the preprocessed Penn Treebank (Wall Street Journal) dataset from here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data.\n",
    "KenLM doesn't support <unk> token so let's remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes all occurences of <unk> tokens\n",
    "# sed is a very handy command for quick processing.  \n",
    "# I strongly recommend you to learn how to use it. \n",
    "# https://www.tutorialspoint.com/sed/sed_overview.htm\n",
    "!sed -e 's/<unk>//g' data/ptb.train.txt > data/ptb.train.nounk.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 842501 types 10001\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:120012 2:6871827456\n",
      "Statistics:\n",
      "1 10001 D1=0.269406 D2=0.72032 D3+=1.15669\n",
      "2 271372 D1=0.717012 D2=1.07895 D3+=1.44702\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 5024 assuming -p 1.5\n",
      "probing 5063 assuming -r models -p 1.5\n",
      "trie    1725 without quantization\n",
      "trie     964 assuming -q 8 -b 8 quantization \n",
      "trie    1725 assuming -a 22 array pointer compression\n",
      "trie     964 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "RSSMax:1894408192 kB\tuser:0.790478\tsys:0.795327\tCPU:1.58586\treal:1.5799\n"
     ]
    }
   ],
   "source": [
    "#bigram\n",
    "!./kenlm/build/bin/lmplz -o 2 < data/ptb.train.nounk.txt > ptb_lm_2gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 842501 types 10001\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:120012 2:1169672704 3:2193136384 4:3509018368\n",
      "Statistics:\n",
      "1 10001 D1=0.269406 D2=0.72032 D3+=1.15669\n",
      "2 271372 D1=0.736147 D2=1.10173 D3+=1.46771\n",
      "3 578317 D1=0.878891 D2=1.26107 D3+=1.45765\n",
      "4 685219 D1=0.930799 D2=1.34496 D3+=1.30068\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 32213 assuming -p 1.5\n",
      "probing 37231 assuming -r models -p 1.5\n",
      "trie    14059 without quantization\n",
      "trie     7265 assuming -q 8 -b 8 quantization \n",
      "trie    12759 assuming -a 22 array pointer compression\n",
      "trie     5965 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952 3:11566340 4:16445256\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952 3:11566340 4:16445256\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "RSSMax:1409093632 kB\tuser:1.63751\tsys:0.831192\tCPU:2.46876\treal:2.17867\n"
     ]
    }
   ],
   "source": [
    "# 4-gram\n",
    "!./kenlm/build/bin/lmplz -o 4 < data/ptb.train.nounk.txt > ptb_lm_4gram.arpa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring using KenLM\n",
    "Let's score a sentence using the language model we just trained.  \n",
    "**Note that the score KenLM returns is log likelihood, not perplexity!**  \n",
    "Pereplexity is defined as follow: $$ PPL = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b q(x_i)} $$  \n",
    "\n",
    "All probabilities here are in log base 10 so to convert to perplexity, we do the following:  \n",
    "$$PPL = 10^{-\\log(P) / N} $$\n",
    "where $-\\log(P)$ is the total NLL of the whole sentence, and $N$ is the word count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained LMs\n",
    "bigram_model = kenlm.LanguageModel('ptb_lm_2gram.arpa')\n",
    "trigram_model = kenlm.LanguageModel('ptb_lm_4gram.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating perplexity\n",
    "def get_ppl(model, sent):\n",
    "    return 10**(-model.score(sent)/len(sent.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"dividend yields have been bolstered by stock declines \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL of a sentence from PTB test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.9773725405043\n",
      "733.1557213309632\n"
     ]
    }
   ],
   "source": [
    "print(get_ppl(bigram_model, sentence))\n",
    "print(get_ppl(trigram_model, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL of an out-of-domain sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13349.78268920608\n",
      "13699.961190363858\n"
     ]
    }
   ],
   "source": [
    "ood_sentence = 'artificial neural networks are computing systems vaguely inspired by the biological neural networks'\n",
    "print(get_ppl(bigram_model, ood_sentence))\n",
    "print(get_ppl(trigram_model, ood_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the sentence from test set to get novel N-grams and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock bolstered declines dividend by yields have been\n",
      "3207.5970808942507\n",
      "3302.2615231292616\n"
     ]
    }
   ],
   "source": [
    "random.seed(555)\n",
    "tmp = sentence.split()\n",
    "random.shuffle(tmp)\n",
    "tmp_sent_2 = ' '.join(tmp)\n",
    "print(tmp_sent_2)\n",
    "print(get_ppl(bigram_model, tmp_sent_2))\n",
    "print(get_ppl(trigram_model, tmp_sent_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that perplexity gets higher, but not as high as the out-of-domain sentence. \n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we know if a word is OOV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not OOV word!\n"
     ]
    }
   ],
   "source": [
    "random_word='tiger'\n",
    "if random_word not in bigram_model:\n",
    "    print('OOV word!')\n",
    "else:\n",
    "    print('not OOV word!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one to two corpora to test the language models on. Feel free to use one of following or any other appropriate corpus that appeals to you,\n",
    "\n",
    "Billion word dataset: http://www.statmt.org/lm-benchmark/  \n",
    "Quaker historical corpus: https://www.woodbrooke.org.uk/resource-library/quaker-historical-corpus/  \n",
    "All of Shakespeare: http://norvig.com/ngrams/  \n",
    "IMDB: http://ai.stanford.edu/~amaas/data/sentiment/  \n",
    "SNLI test set and MultiNLI dev-set (only hypothesis sentences) in data folder.\n",
    "\n",
    "**Exercise 1**: Load the data and get the perpelxity.\n",
    "\n",
    "**Exercise 2**: How many OOV words are there?\n",
    "\n",
    "**Exercise 3**: Find the sentence with the highest and lowest perplexity.\n",
    "\n",
    "**Exercise 4**: Do you think vocabulary size affect the perplexity of language model? Explore this by removing some words from training corpus.\n",
    "\n",
    "**Exercise 5**: If you want to train a model on a larger dataset, follow the directions on the KenLM website, and see how this new model fares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
