{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1011 Fall 2018 Lab 5\n",
    "# Intrinsic Evaluation of Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure you've downloaded the following: \n",
    "1. [GloVe vectors](https://nlp.stanford.edu/projects/glove/): We'll use the 6B, 50D version so download glove.6B.zip (822MB) from the website (or `wget http://nlp.stanford.edu/data/glove.6B.zip` )\n",
    "2. [fastText vectors](https://fasttext.cc/docs/en/english-vectors.html): We'll use the 1M, 300D version (650MB) (`wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a set of 50D word vectors from GloVe. `glove_home` below specifies the location of the unzipped file. `words_to_load` specifies how many word vectors we want to load. The words are saved in frequency order, so specifying 50,000 means that we only want to work with the 50,000 most frequent words from the source corpus. You can load up to 400,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rongfeng/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    loaded_embeddings = np.zeros((words_to_load, 50))\n",
    "    words = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "        words[s[0]] = i\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to look up a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.063054 -0.62636  -0.76417  -0.041484  0.56284   0.86432  -0.73734\n",
      " -0.70925  -0.073065 -0.74619  -0.34769   0.14402   1.4576    0.034688\n",
      "  0.11224   0.13854   0.10484   0.60207   0.021777 -0.21802   0.087613\n",
      " -1.4234    1.0361    0.1509    0.13608  -0.2971   -0.90828   0.34182\n",
      "  1.3367    0.16329   1.2374   -0.20113  -0.91532   1.4222   -0.1276\n",
      "  0.69443  -1.1782    1.2072    1.0524   -0.11957  -0.1275    0.41798\n",
      " -0.9232   -0.1312    1.2696    1.2318    0.30061  -0.18854   0.15899\n",
      "  0.0486  ]\n"
     ]
    }
   ],
   "source": [
    "# loaded_embeddings: original embedding matrix, dim = (words_to_load, 50)\n",
    "# words: a dictionary that maps word to its idx\n",
    "# idx2words: a dictionary that maps idx to word\n",
    "print(loaded_embeddings[words['potato']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Similarity Measure\n",
    "\n",
    "Implement the function dot_similarity that returns the same similarity as the cosine_similarity in sklearn for the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7964893661716318\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-3.5586e-01  5.2130e-01 -6.1070e-01 -3.0131e-01  9.4862e-01 -3.1539e-01\n -5.9831e-01  1.2188e-01 -3.1943e-02  5.5695e-01 -1.0621e-01  6.3399e-01\n -4.7340e-01 -7.5895e-02  3.8247e-01  8.1569e-02  8.2214e-01  2.2220e-01\n -8.3764e-03 -7.6620e-01 -5.6253e-01  6.1759e-01  2.0292e-01 -4.8598e-02\n  8.7815e-01 -1.6549e+00 -7.7418e-01  1.5435e-01  9.4823e-01 -3.9520e-01\n  3.7302e+00  8.2855e-01 -1.4104e-01  1.6395e-02  2.1115e-01 -3.6085e-02\n -1.5587e-01  8.6583e-01  2.6309e-01 -7.1015e-01 -3.6770e-02  1.8282e-03\n -1.7704e-01  2.7032e-01  1.1026e-01  1.4133e-01 -5.7322e-02  2.7207e-01\n  3.1305e-01  9.2771e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-78d27eeba180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Your handcraft_cosine_similarity should give (almost) same values as sklearn_cosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandcraft_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandcraft_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"well\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-78d27eeba180>\u001b[0m in \u001b[0;36msklearn_cosine_similarity\u001b[0;34m(vec_one, vec_two)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcalculates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcosine\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 111\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    112\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[1;32m    113\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-3.5586e-01  5.2130e-01 -6.1070e-01 -3.0131e-01  9.4862e-01 -3.1539e-01\n -5.9831e-01  1.2188e-01 -3.1943e-02  5.5695e-01 -1.0621e-01  6.3399e-01\n -4.7340e-01 -7.5895e-02  3.8247e-01  8.1569e-02  8.2214e-01  2.2220e-01\n -8.3764e-03 -7.6620e-01 -5.6253e-01  6.1759e-01  2.0292e-01 -4.8598e-02\n  8.7815e-01 -1.6549e+00 -7.7418e-01  1.5435e-01  9.4823e-01 -3.9520e-01\n  3.7302e+00  8.2855e-01 -1.4104e-01  1.6395e-02  2.1115e-01 -3.6085e-02\n -1.5587e-01  8.6583e-01  2.6309e-01 -7.1015e-01 -3.6770e-02  1.8282e-03\n -1.7704e-01  2.7032e-01  1.1026e-01  1.4133e-01 -5.7322e-02  2.7207e-01\n  3.1305e-01  9.2771e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "def sklearn_cosine_similarity(vec_one, vec_two):\n",
    "    \"\"\"\n",
    "    Function that calculates the cosine similarity between two words\n",
    "    \"\"\"\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "\n",
    "def handcraft_cosine_similarity(vec_one, vec_two):\n",
    "    \"\"\"\n",
    "    Function that calculates the cosine similarity between two words\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    # a dot b / (norm a * normal b)\n",
    "    norm_one = np.sqrt(np.sum(vec_one**2))\n",
    "    norm_two = np.sqrt(np.sum(vec_two**2))\n",
    "    return vec_one.dot(vec_two)/norm_one/norm_two\n",
    "\n",
    "# Your handcraft_cosine_similarity should give (almost) same values as sklearn_cosine_similarity\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"bad\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"bad\"]]))\n",
    "\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"well\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"well\"]]))\n",
    "\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"fish\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"fish\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Semantic Orientation Method\n",
    "\n",
    "The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension.\n",
    "\n",
    "*Some code in this section was adapted from Stanford CS 224U*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_coefficient(candidate_word, loaded_embeddings):\n",
    "    # Here's a sample pair of seed sets:\n",
    "    seed_pos = ['table', 'chair', 'lamp', 'curtain', 'desk']\n",
    "    seed_neg = ['fish', 'bird', 'dog', 'cat', 'cow']\n",
    "    \n",
    "    # Let's look up the embeddings for these words.\n",
    "    seed_pos_indices = [words[seed] for seed in seed_pos]\n",
    "    seed_neg_indices = [words[seed] for seed in seed_neg]\n",
    "    seed_pos_mat = loaded_embeddings[seed_pos_indices]\n",
    "    seed_neg_mat = loaded_embeddings[seed_neg_indices]\n",
    "\n",
    "    # Scoring words along the axis\n",
    "    candidate = loaded_embeddings[words[candidate_word]]\n",
    "    pos_sim = np.sum([cosine_similarity(np.array([candidate,reference]))[0,1] for reference in seed_pos_mat])\n",
    "    neg_sim = np.sum([cosine_similarity(np.array([candidate,reference]))[0,1] for reference in seed_neg_mat])\n",
    "    return pos_sim - neg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2916448442212511"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "determine_coefficient('abhorrent', loaded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sort our vocabulary by its score along the axis. For now, we're only scoring frequent words, since this process can be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_words = [(word, determine_coefficient(word, loaded_embeddings)) for word in ordered_words[1:10000]]\n",
    "sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('panels', 2.088893133875392),\n",
      "    ('desk', 2.031519353296948),\n",
      "    ('chairs', 1.9969309470439889),\n",
      "    ('chair', 1.9807613618158872),\n",
      "    ('slobodan', 1.9798640819000615),\n",
      "    ('ceiling', 1.9240190533444927),\n",
      "    ('doors', 1.920482459380041),\n",
      "    ('rotating', 1.885553745623736),\n",
      "    ('belgrade', 1.8764716706543159),\n",
      "    ('columns', 1.8563476943420172)]\n",
      "[   ('cow', -2.9569226842503786),\n",
      "    ('breeding', -2.979690822318722),\n",
      "    ('breed', -2.9859391768769226),\n",
      "    ('bird', -3.065702288819476),\n",
      "    ('cats', -3.1399804835837357),\n",
      "    ('cattle', -3.1442049121562583),\n",
      "    ('whale', -3.1587269448292594),\n",
      "    ('shark', -3.2280199929166615),\n",
      "    ('sheep', -3.257317308876691),\n",
      "    ('pigs', -3.3829592988198574)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(sorted_words[:10])\n",
    "pp.pprint(sorted_words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend a few minutes exploring possible seed sets for other semantic dimensions. What works? What doesn't? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Word Analogies\n",
    "\n",
    "\n",
    "The word analogy task consists of questions like, “a is to b as c is to ?” As mentioned in the GloVe paper, the answer to this problem is the word that gives the max cosine similarity for equation emb(b) − emb(a) + emb(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def find_nearest_word(input_vec, k=5):\n",
    "    \"\"\"\n",
    "    Function that returns the top k words whose embedding has the smallest cosine distance to the input_vec\n",
    "    @param input_vec: embedding for a single word\n",
    "    @param k: top k neighbours to return\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    return None\n",
    "\n",
    "\n",
    "def word_analogy(word_a, word_b, word_c, k=5):\n",
    "    \"\"\"\n",
    "    Function that solves problem word_a to word_b = word_c to ?\n",
    "    @param word_a, word_b, word_c: string\n",
    "    @param k: top k candidates to return\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    return None\n",
    "\n",
    "\n",
    "# embedding algebra\n",
    "print(find_nearest_word(loaded_embeddings[words[\"student\"]] - loaded_embeddings[words[\"study\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings[words[\"working-class\"]] + loaded_embeddings[words[\"money\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings[words[\"drunk\"]] - loaded_embeddings[words[\"alcohol\"]], k=5))\n",
    "\n",
    "\n",
    "# Analogy\n",
    "print(word_analogy(\"china\", \"chinese\", \"america\"))\n",
    "print(word_analogy(\"china\", \"beijing\", \"america\"))\n",
    "print(word_analogy(\"king\", \"male\", \"queen\"))\n",
    "print(word_analogy(\"athens\", \"greece\", \"berlin\"))\n",
    "print(word_analogy(\"dark\", \"darker\", \"soft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try loading Fast text vectors and analyse them in a similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the cosine similarity between fT vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.500e-03, -1.100e-02,  5.810e-02,  6.060e-02, -9.000e-04,\n",
       "        9.800e-02, -1.930e-02,  9.730e-02,  6.300e-03, -6.600e-03,\n",
       "       -3.090e-02,  9.300e-03,  1.200e-02, -7.790e-02,  4.330e-02,\n",
       "        4.650e-02, -1.490e-02, -1.457e-01,  5.580e-02,  5.610e-02,\n",
       "       -1.539e-01, -4.880e-02, -8.130e-02, -7.100e-03,  1.345e-01,\n",
       "       -6.610e-02, -4.060e-02,  1.124e-01, -7.260e-02, -3.500e-02,\n",
       "       -3.220e-02, -6.680e-02, -3.180e-02, -4.260e-02,  1.000e-02,\n",
       "        5.240e-02, -5.300e-03,  7.020e-02, -7.530e-02, -4.600e-03,\n",
       "        5.930e-02,  1.360e-02,  4.630e-02,  1.446e-01, -3.420e-02,\n",
       "        1.670e-02, -1.590e-02,  2.390e-02,  8.080e-02, -3.490e-02,\n",
       "        5.280e-02,  4.960e-02, -6.050e-01, -2.930e-02,  6.500e-03,\n",
       "       -7.400e-03,  4.090e-02,  2.520e-02, -3.400e-03, -2.980e-02,\n",
       "        3.600e-02, -3.200e-03,  1.790e-02,  3.660e-02,  7.870e-02,\n",
       "       -5.620e-02, -6.760e-02, -4.300e-03,  2.480e-02,  1.240e-02,\n",
       "       -1.200e-01,  1.058e-01,  3.980e-02,  3.920e-02,  2.760e-02,\n",
       "        1.165e-01, -8.840e-02, -2.480e-02, -3.590e-02,  4.160e-02,\n",
       "        2.430e-02,  1.154e-01,  1.036e-01, -2.890e-01,  4.230e-02,\n",
       "       -2.000e-02, -2.840e-02,  3.370e-02,  9.650e-02, -1.710e-01,\n",
       "       -8.390e-02, -1.194e-01, -9.500e-03,  8.200e-03, -2.020e-02,\n",
       "        1.287e-01,  9.230e-02, -5.950e-02, -1.096e-01,  8.760e-02,\n",
       "       -1.225e-01,  6.900e-02,  8.660e-02, -1.100e-01,  1.990e-02,\n",
       "       -4.700e-03, -1.429e-01, -8.960e-02,  4.270e-02,  5.510e-02,\n",
       "       -9.420e-02,  9.780e-02, -9.720e-02, -8.000e-04, -2.110e-02,\n",
       "       -6.470e-02,  6.060e-02,  1.359e-01, -2.020e-02, -3.276e-01,\n",
       "        6.600e-03, -1.495e-01,  7.000e-04,  2.120e-02,  1.900e-02,\n",
       "        1.661e-01, -5.580e-02, -2.120e-02,  3.430e-02,  2.520e-02,\n",
       "        1.035e-01, -7.100e-02, -4.000e-02, -1.930e-02, -1.160e-02,\n",
       "        1.700e-02, -1.299e-01, -1.368e-01,  5.690e-02,  2.820e-02,\n",
       "       -4.680e-02,  3.300e-03, -7.990e-02,  1.675e-01, -3.900e-02,\n",
       "       -1.207e-01,  2.700e-03, -4.060e-02,  8.630e-02,  9.560e-02,\n",
       "        1.450e-02,  2.580e-02, -6.570e-02, -1.028e-01,  8.390e-02,\n",
       "       -6.870e-02, -8.610e-02, -5.000e-03, -1.490e-02,  7.460e-02,\n",
       "        3.300e-02, -3.300e-03,  1.700e-03, -1.000e-03, -1.870e-02,\n",
       "       -4.320e-02,  4.090e-02, -7.350e-02,  6.150e-02,  1.146e-01,\n",
       "       -5.630e-02,  5.600e-03, -4.040e-02, -6.930e-02, -1.510e-02,\n",
       "       -1.800e-02,  2.627e-01, -1.274e-01,  7.420e-02,  6.010e-02,\n",
       "       -1.340e-02, -5.920e-02, -5.900e-03,  4.130e-02,  7.580e-02,\n",
       "       -2.870e-02,  7.720e-02,  2.310e-02, -1.953e-01,  3.680e-02,\n",
       "       -1.000e-03,  1.620e-02,  2.140e-02,  1.620e-01,  1.272e-01,\n",
       "       -1.387e-01,  2.350e-02,  6.160e-02,  1.200e-01,  1.348e-01,\n",
       "       -1.131e-01, -9.550e-02, -6.470e-02, -3.740e-02,  1.178e-01,\n",
       "        1.247e-01, -6.490e-02, -1.687e-01,  5.330e-02,  2.510e-02,\n",
       "        5.350e-02,  1.997e-01,  5.100e-02, -7.300e-03, -2.100e-02,\n",
       "       -4.360e-02, -2.110e-02,  2.930e-02, -1.550e-02, -4.420e-02,\n",
       "       -4.580e-02,  3.650e-02,  3.240e-02, -7.060e-02,  1.106e-01,\n",
       "       -1.847e-01,  6.440e-02,  5.000e-04, -1.153e-01, -1.160e-02,\n",
       "       -8.190e-02, -4.500e-02,  4.009e-01,  7.730e-02, -5.710e-02,\n",
       "       -1.277e-01, -2.935e-01,  1.848e-01, -2.529e-01, -1.639e-01,\n",
       "        8.400e-02,  2.960e-02,  2.290e-02, -7.750e-02, -9.380e-02,\n",
       "       -1.730e-02,  3.740e-02,  1.114e-01, -3.100e-03,  3.600e-01,\n",
       "        9.800e-02,  1.055e-01, -8.310e-02, -7.900e-03,  9.870e-02,\n",
       "        1.010e-01, -4.670e-02, -3.080e-02, -9.090e-02,  1.390e-01,\n",
       "        6.110e-02,  1.090e-02, -5.700e-02, -1.551e-01, -4.909e-01,\n",
       "        3.370e-02, -4.840e-02, -1.800e-03, -1.843e-01,  7.120e-02,\n",
       "        1.000e-04,  6.910e-02, -9.200e-02, -5.390e-02, -2.100e-02,\n",
       "       -7.120e-02, -7.400e-03, -1.022e-01, -1.240e-01,  1.230e-01,\n",
       "        4.070e-02,  5.070e-02,  4.130e-02,  2.044e-01,  4.270e-02,\n",
       "       -8.310e-02, -1.144e-01,  1.604e-01, -1.300e-02,  3.950e-02,\n",
       "        3.100e-03,  6.210e-02, -8.170e-02,  8.060e-02,  6.250e-02,\n",
       "        2.940e-02,  4.150e-02,  5.570e-02,  2.520e-02, -8.980e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_embeddings_ft[words[\"good\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5e5685bf4e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"well\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_embeddings_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fish\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-78ad95bad7f2>\u001b[0m in \u001b[0;36msklearn_cosine_similarity\u001b[0;34m(vec_one, vec_two)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcalculates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcosine\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec_one\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec_two\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         X = Y = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 108\u001b[0;31m                             warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"bad\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"well\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"fish\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the semantic orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_words = [(word, determine_coefficient(word, loaded_embeddings_ft)) for word in ordered_words[1:10000]]\n",
    "sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)\n",
    "pp.pprint(sorted_words[:10])\n",
    "pp.pprint(sorted_words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(find_nearest_word(loaded_embeddings_ft[words[\"student\"]] - loaded_embeddings_ft[words[\"study\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings_ft[words[\"working-class\"]] + loaded_embeddings_ft[words[\"money\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings_ft[words[\"drunk\"]] - loaded_embeddings_ft[words[\"alcohol\"]], k=5))\n",
    "\n",
    "\n",
    "# Analogy\n",
    "print(word_analogy(\"china\", \"chinese\", \"america\"))\n",
    "print(word_analogy(\"china\", \"beijing\", \"america\"))\n",
    "print(word_analogy(\"king\", \"male\", \"queen\"))\n",
    "print(word_analogy(\"athens\", \"greece\", \"berlin\"))\n",
    "print(word_analogy(\"dark\", \"darker\", \"soft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Visualize word vectors (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: TSNE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More questions to think about:\n",
    "- Can we analyse and quantify the difference in Glove and fastText vectors?\n",
    "- If we only care about the nearest neighbour in a fixed set, will the neighbour with smallest L2 distance be the same neighbour that gives the max cosine similarity?\n",
    "- Will we lose any information about embeddings if we normalize the embedding vectors? Why?\n",
    "- Is cosine distance (1/cosine similarity) a valid distance metrics? Why?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
